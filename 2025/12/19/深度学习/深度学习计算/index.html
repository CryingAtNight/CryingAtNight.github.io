<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习计算 | Yinjin Yao的博客</title><meta name="author" content="Yinjin Yao"><meta name="copyright" content="Yinjin Yao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="层和块 :label:sec_model_construction 之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。在这里，整个模型只有一个输出。注意，单个神经网络（1）接受一些输入；（2）生成相应的标量输出；（3）具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。 然后，当考虑具有多个输出的网络时，我们利用矢量化算法来描述整层神经元。像单个神经元一样，层（1">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习计算">
<meta property="og:url" content="https://cryingatnight.github.io/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/index.html">
<meta property="og:site_name" content="Yinjin Yao的博客">
<meta property="og:description" content="层和块 :label:sec_model_construction 之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。在这里，整个模型只有一个输出。注意，单个神经网络（1）接受一些输入；（2）生成相应的标量输出；（3）具有一组相关 参数（parameters），更新这些参数可以优化某目标函数。 然后，当考虑具有多个输出的网络时，我们利用矢量化算法来描述整层神经元。像单个神经元一样，层（1">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cryingatnight.github.io/img/lita2.jpg">
<meta property="article:published_time" content="2025-12-18T16:00:00.000Z">
<meta property="article:modified_time" content="2025-12-19T09:19:10.147Z">
<meta property="article:author" content="Yinjin Yao">
<meta property="article:tag" content="deeplearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cryingatnight.github.io/img/lita2.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习计算",
  "url": "https://cryingatnight.github.io/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/",
  "image": "https://cryingatnight.github.io/img/lita2.jpg",
  "datePublished": "2025-12-18T16:00:00.000Z",
  "dateModified": "2025-12-19T09:19:10.147Z",
  "author": [
    {
      "@type": "Person",
      "name": "Yinjin Yao",
      "url": "https://cryingatnight.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/headimage.png"><link rel="canonical" href="https://cryingatnight.github.io/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习计算',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/headimage.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-flask"></i><span> 实验室</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="http://121.199.61.105/"><i class="fa-fw fas fa-q"></i><span> 豆瓣网开发</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fuckornot.on.websim.com/"><i class="fa-fw fa fa-trophy"></i><span> 上不上AI评分系统</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fontawesome.com/icons"><i class="fa-fw fa fa-check-circle"></i><span> font-awesome v6 图标</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/lita2.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/headimage.png" alt="Logo"><span class="site-name">Yinjin Yao的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习计算</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-flask"></i><span> 实验室</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="http://121.199.61.105/"><i class="fa-fw fas fa-q"></i><span> 豆瓣网开发</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fuckornot.on.websim.com/"><i class="fa-fw fa fa-trophy"></i><span> 上不上AI评分系统</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fontawesome.com/icons"><i class="fa-fw fa fa-check-circle"></i><span> font-awesome v6 图标</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深度学习计算</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-18T16:00:00.000Z" title="发表于 2025-12-19 00:00:00">2025-12-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-19T09:19:10.147Z" title="更新于 2025-12-19 17:19:10">2025-12-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h1><hr>
<p>:label:<code>sec_model_construction</code></p>
<p>之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。<br>在这里，整个模型只有一个输出。<br>注意，单个神经网络<br>（1）接受一些输入；<br>（2）生成相应的标量输出；<br>（3）具有一组相关 <em>参数</em>（parameters），更新这些参数可以优化某目标函数。</p>
<p>然后，当考虑具有多个输出的网络时，<br>我们利用矢量化算法来描述整层神经元。<br>像单个神经元一样，层（1）接受一组输入，<br>（2）生成相应的输出，<br>（3）由一组可调整参数描述。<br>当我们使用softmax回归时，一个单层本身就是模型。<br>然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。</p>
<p>对于多层感知机而言，整个模型及其组成层都是这种架构。<br>整个模型接受原始输入（特征），生成输出（预测），<br>并包含一些参数（所有组成层的参数集合）。<br>同样，每个单独的层接收输入（由前一层提供），<br>生成输出（到下一层的输入），并且具有一组可调参数，<br>这些参数根据从下一层反向传播的信号进行更新。</p>
<p>事实证明，研究<strong>讨论“比单个层大”但“比整个模型小”的组件更有价值</strong>。<br>例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，<br>这些层是由<em>层组</em>（groups of layers）的重复模式组成。<br>这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛<br>的识别和检测任务 :cite:<code>He.Zhang.Ren.ea.2016</code>。<br>目前ResNet架构仍然是许多视觉任务的首选架构。<br>在其他的领域，如自然语言处理和语音，<br>层组以各种重复模式排列的类似架构现在也是普遍存在。</p>
<p>为了实现这些复杂的网络，我们引入了神经网络<em>块</em>的概念。<br><em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。<br>使用块进行抽象的一个好处是可以将一些块组合成更大的组件，<br>这一过程通常是递归的，如 :numref:<code>fig_blocks</code>所示。<br>通过定义代码来按需生成任意复杂度的块，<br>我们可以通过简洁的代码实现复杂的神经网络。</p>
<p><img src="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/blocks.svg" alt="多个层被组合成块，形成更大的模型"><br>:label:<code>fig_blocks</code></p>
<p>从编程的角度来看，块由<em>类</em>（class）表示。<br>它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，<br>并且必须存储任何必需的参数。<br>注意，有些块不需要任何参数。<br>最后，为了计算梯度，块必须具有反向传播函数。<br>在定义我们自己的块时，由于自动微分（在 :numref:<code>sec_autograd</code> 中引入）<br>提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。</p>
<p>在构造自定义块之前，(<strong>我们先回顾一下多层感知机</strong>)<br>（ :numref:<code>sec_mlp_concise</code> ）的代码。<br>下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，<br>然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F  </span><br><span class="line">  </span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))  </span><br><span class="line">  </span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)  </span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0343,  0.0264,  0.2505, -0.0243,  0.0945,  0.0012, -0.0141,  0.0666,</span><br><span class="line">         -0.0547, -0.0667],</span><br><span class="line">        [ 0.0772, -0.0274,  0.2638, -0.0191,  0.0394, -0.0324,  0.0102,  0.0707,</span><br><span class="line">         -0.1481, -0.1031]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>在这个例子中，我们通过实例化<code>nn.Sequential</code>来构建我们的模型，<br>层的执行顺序是作为参数传递的。<br>简而言之，(<strong><code>nn.Sequential</code>定义了一种特殊的<code>Module</code></strong>)，<br>即在PyTorch中表示一个块的类，<br>它维护了一个由<code>Module</code>组成的有序列表。<br>注意，两个全连接层都是<code>Linear</code>类的实例，<br><code>Linear</code>类本身就是<code>Module</code>的子类。<br>另外，到目前为止，我们一直在通过<code>net(X)</code>调用我们的模型来获得模型的输出。<br>这实际上是<code>net.__call__(X)</code>的简写。<br>这个前向传播函数非常简单：<br>它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。</p>
<h2 id="自定义块"><a href="#自定义块" class="headerlink" title="[自定义块]"></a>[<strong>自定义块</strong>]</h2><p>要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。<br>在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。</p>
<ol>
<li>将输入数据作为其前向传播函数的参数。  </li>
<li>通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。  </li>
<li>计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。  </li>
<li>存储和访问前向传播计算所需的参数。  </li>
<li>根据需要初始化模型参数。</li>
</ol>
<p>在下面的代码片段中，我们从零开始编写一个块。<br>它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。<br>注意，下面的<code>MLP</code>类继承了表示块的类。<br>我们的实现只需要提供我们自己的构造函数（Python中的<code>__init__</code>函数）和前向传播函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):  </span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层  </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。  </span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）  </span></span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层  </span></span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层  </span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出  </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  </span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。  </span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out(F.relu(<span class="variable language_">self</span>.hidden(X)))</span><br></pre></td></tr></table></figure>


<p>我们首先看一下前向传播函数，它以<code>X</code>作为输入，<br>计算带有激活函数的隐藏表示，并输出其未规范化的输出值。<br>在这个<code>MLP</code>实现中，两个层都是实例变量。<br>要了解这为什么是合理的，可以想象实例化两个多层感知机（<code>net1</code>和<code>net2</code>），<br>并根据不同的数据对它们进行训练。<br>当然，我们希望它们学到两种不同的模型。  </p>
<p>接着我们[<strong>实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层</strong>]。<br>注意一些关键细节：<br>首先，我们定制的<code>__init__</code>函数通过<code>super().__init__()</code><br>调用父类的<code>__init__</code>函数，<br>省去了重复编写模版代码的痛苦。<br>然后，我们实例化两个全连接层，<br>分别为<code>self.hidden</code>和<code>self.out</code>。<br>注意，除非我们实现一个新的运算符，<br>否则我们不必担心反向传播函数或参数初始化，<br>系统将自动生成这些。  </p>
<p>我们来试一下这个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MLP()  </span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0669,  0.2202, -0.0912, -0.0064,  0.1474, -0.0577, -0.3006,  0.1256,</span><br><span class="line">         -0.0280,  0.4040],</span><br><span class="line">        [ 0.0545,  0.2591, -0.0297,  0.1141,  0.1887,  0.0094, -0.2686,  0.0732,</span><br><span class="line">         -0.0135,  0.3865]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>


<p>块的一个主要优点是它的多功能性。<br>我们可以子类化块以创建层（如全连接层的类）、<br>整个模型（如上面的<code>MLP</code>类）或具有中等复杂度的各种组件。<br>我们在接下来的章节中充分利用了这种多功能性，<br>比如在处理卷积神经网络时。  </p>
<h2 id="顺序块"><a href="#顺序块" class="headerlink" title="[顺序块]"></a>[<strong>顺序块</strong>]</h2><p>现在我们可以更仔细地看看<code>Sequential</code>类是如何工作的，<br>回想一下<code>Sequential</code>的设计是为了把其他模块串起来。<br>为了构建我们自己的简化的<code>MySequential</code>，<br>我们只需要定义两个关键函数：  </p>
<ol>
<li>一种将块逐个追加到列表中的函数；  </li>
<li>一种前向传播函数，用于<strong>将输入按追加块的顺序传递给块组成的“链条”</strong>。</li>
</ol>
<p>下面的<code>MySequential</code>类提供了与默认<code>Sequential</code>类相同的功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):  </span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员  </span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict  </span></span><br><span class="line">            <span class="variable language_">self</span>._modules[<span class="built_in">str</span>(idx)] = module  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  </span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们  </span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>._modules.values():  </span><br><span class="line">            X = block(X)  </span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>

<p><code>__init__</code>函数将每个模块逐个添加到有序字典<code>_modules</code>中。<br>读者可能会好奇为什么每个<code>Module</code>都有一个<code>_modules</code>属性？<br>以及为什么我们使用它而不是自己定义一个Python列表？<br>简而言之，<code>_modules</code>的主要优点是：<br>在模块的参数初始化过程中，<br>系统知道在<code>_modules</code>字典中查找需要初始化参数的子块。</p>
<p>当<code>MySequential</code>的前向传播函数被调用时，<br>每个添加的块都按照它们被添加的顺序执行。<br>现在可以使用我们的<code>MySequential</code>类重新实现多层感知机。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))  </span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 2.2759e-01, -4.7003e-02,  4.2846e-01, -1.2546e-01,  1.5296e-01,</span><br><span class="line">          1.8972e-01,  9.7048e-02,  4.5479e-04, -3.7986e-02,  6.4842e-02],</span><br><span class="line">        [ 2.7825e-01, -9.7517e-02,  4.8541e-01, -2.4519e-01, -8.4580e-02,</span><br><span class="line">          2.8538e-01,  3.6861e-02,  2.9411e-02, -1.0612e-01,  1.2620e-01]],</span><br><span class="line">       grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>请注意，<code>MySequential</code>的用法与之前为<code>Sequential</code>类编写的代码相同<br>（如 :numref:<code>sec_mlp_concise</code> 中所述）。</p>
<h2 id="在前向传播函数中执行代码"><a href="#在前向传播函数中执行代码" class="headerlink" title="[在前向传播函数中执行代码]"></a>[<strong>在前向传播函数中执行代码</strong>]</h2><p><code>Sequential</code>类使模型构造变得简单，<br>允许我们组合新的架构，而不必定义自己的类。<br>然而，并不是所有的架构都是简单的顺序架构。<br>当需要更强的灵活性时，我们需要定义自己的块。<br>例如，我们可能希望在前向传播函数中执行Python的控制流。<br>此外，我们可能希望执行任意的数学运算，<br>而不是简单地依赖预定义的神经网络层。</p>
<p>到目前为止，<br>我们网络中的所有操作都对网络的激活值及网络的参数起作用。<br>然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，<br>我们称之为<em>常数参数</em>（constant parameter）。<br>例如，我们需要一个计算函数<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="17.581ex" height="2.456ex" role="img" focusable="false" viewBox="0 -835.3 7770.8 1085.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(939,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g><g data-mml-node="mo" transform="translate(1546,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1990.7,0)"><g data-mml-node="mi"><path data-c="1D430" d="M624 444Q636 441 722 441Q797 441 800 444H805V382H741L593 11Q592 10 590 8T586 4T584 2T581 0T579 -2T575 -3T571 -3T567 -4T561 -4T553 -4H542Q525 -4 518 6T490 70Q474 110 463 137L415 257L367 137Q357 111 341 72Q320 17 313 7T289 -4H277Q259 -4 253 -2T238 11L90 382H25V444H32Q47 441 140 441Q243 441 261 444H270V382H222L310 164L382 342L366 382H303V444H310Q322 441 407 441Q508 441 523 444H531V382H506Q481 382 481 380Q482 376 529 259T577 142L674 382H617V444H624Z"></path></g></g><g data-mml-node="mo" transform="translate(2821.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3488.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4544.2,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(5199.4,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msup" transform="translate(5699.7,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D430" d="M624 444Q636 441 722 441Q797 441 800 444H805V382H741L593 11Q592 10 590 8T586 4T584 2T581 0T579 -2T575 -3T571 -3T567 -4T561 -4T553 -4H542Q525 -4 518 6T490 70Q474 110 463 137L415 257L367 137Q357 111 341 72Q320 17 313 7T289 -4H277Q259 -4 253 -2T238 11L90 382H25V444H32Q47 441 140 441Q243 441 261 444H270V382H222L310 164L382 342L366 382H303V444H310Q322 441 407 441Q508 441 523 444H531V382H506Q481 382 481 380Q482 376 529 259T577 142L674 382H617V444H624Z"></path></g></g><g data-mml-node="mi" transform="translate(864,363) scale(0.707)"><path data-c="22A4" d="M55 642T55 648T59 659T66 666T71 668H708Q723 660 723 648T708 628H409V15Q402 2 391 0Q387 0 384 1T379 3T375 6T373 9T371 13T369 16V628H71Q70 628 67 630T59 637Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7163.8,0)"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g></g></g></svg></mjx-container>的层，<br>其中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.373ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 607 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D431" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path></g></g></g></g></svg></mjx-container>是输入，<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.88ex" height="1.005ex" role="img" focusable="false" viewBox="0 -444 831 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D430" d="M624 444Q636 441 722 441Q797 441 800 444H805V382H741L593 11Q592 10 590 8T586 4T584 2T581 0T579 -2T575 -3T571 -3T567 -4T561 -4T553 -4H542Q525 -4 518 6T490 70Q474 110 463 137L415 257L367 137Q357 111 341 72Q320 17 313 7T289 -4H277Q259 -4 253 -2T238 11L90 382H25V444H32Q47 441 140 441Q243 441 261 444H270V382H222L310 164L382 342L366 382H303V444H310Q322 441 407 441Q508 441 523 444H531V382H506Q481 382 481 380Q482 376 529 259T577 142L674 382H617V444H624Z"></path></g></g></g></g></svg></mjx-container>是参数，<br><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.98ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 433 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></svg></mjx-container>是某个在优化过程中没有更新的指定常量。<br>因此我们实现了一个<code>FixedHiddenMLP</code>类，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="comment"># 不计算梯度的随机权重参数。因此其在训练期间保持不变  </span></span><br><span class="line">        <span class="variable language_">self</span>.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)  </span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  </span><br><span class="line">        X = <span class="variable language_">self</span>.linear(X)  </span><br><span class="line">        <span class="comment"># 使用创建的常量参数以及relu和mm函数  </span></span><br><span class="line">        X = F.relu(torch.mm(X, <span class="variable language_">self</span>.rand_weight) + <span class="number">1</span>)  </span><br><span class="line">        <span class="comment"># 复用全连接层。这相当于两个全连接层共享参数  </span></span><br><span class="line">        X = <span class="variable language_">self</span>.linear(X)  </span><br><span class="line">        <span class="comment"># 控制流  </span></span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:  </span><br><span class="line">            X /= <span class="number">2</span>  </span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>

<p>在这个<code>FixedHiddenMLP</code>模型中，我们实现了一个隐藏层，<br>其权重（<code>self.rand_weight</code>）在实例化时被随机初始化，之后为常量。<br>这个权重不是一个模型参数，因此它永远不会被反向传播更新。<br>然后，神经网络将这个固定层的输出通过一个全连接层。  </p>
<p>注意，在返回输出之前，模型做了一些不寻常的事情：<br>它运行了一个while循环，在<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex;" xmlns="http://www.w3.org/2000/svg" width="2.528ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1117.6 833"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mn" transform="translate(714,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>范数大于<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 500 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>的条件下，<br>将输出向量除以<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 500 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></svg></mjx-container>，直到它满足条件为止。<br>最后，模型返回了<code>X</code>中所有项的和。<br>注意，此操作可能不会常用于在任何实际任务中，<br>我们只展示如何将任意代码集成到神经网络计算的流程中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = FixedHiddenMLP()  </span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(-0.4027, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>


<p>我们可以[<strong>混合搭配各种组合块的方法</strong>]。<br>在下面的例子中，我们以一些想到的方法嵌套块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="variable language_">self</span>.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),  </span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())  </span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(<span class="variable language_">self</span>.net(X))  </span><br><span class="line">  </span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())  </span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(-0.1232, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>读者可能会开始担心操作效率的问题。<br>毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、<br>代码执行和许多其他的Python代码。<br>Python的问题<a target="_blank" rel="noopener" href="https://wiki.python.org/moin/GlobalInterpreterLock">全局解释器锁</a><br>是众所周知的。<br>在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>一个块可以由许多层组成；一个块可以由许多块组成。  </li>
<li>块可以包含代码。  </li>
<li>块负责大量的内部处理，包括参数初始化和反向传播。  </li>
<li>层和块的顺序连接由<code>Sequential</code>块处理。</li>
</ul>
<h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><h3 id="1-如果将-MySequential-中存储块的方式更改为-Python-列表，会出现什么问题？"><a href="#1-如果将-MySequential-中存储块的方式更改为-Python-列表，会出现什么问题？" class="headerlink" title="1. 如果将 MySequential 中存储块的方式更改为 Python 列表，会出现什么问题？"></a><strong>1. 如果将 <code>MySequential</code> 中存储块的方式更改为 Python 列表，会出现什么问题？</strong></h3><p> ✅ 背景回顾：</p>
<p>在李沐的《动手学深度学习》中，<code>MySequential</code> 通常继承自 <code>nn.Module</code>，并使用 <code>add_module()</code> 或直接赋值为属性来注册子模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="variable language_">self</span>.add_module(<span class="built_in">str</span>(idx), module)  <span class="comment"># ← 关键：注册到 Module 的参数系统</span></span><br></pre></td></tr></table></figure>

<p> ❌ 如果改为 Python 列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BadSequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.modules_list = <span class="built_in">list</span>(args)  <span class="comment"># ← 仅作为普通 Python 列表存储</span></span><br></pre></td></tr></table></figure>

<p> ⚠️ 会出现以下严重问题：</p>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
</tr>
</thead>
<tbody><tr>
<td><strong>参数无法被优化器发现</strong></td>
<td><code>optimizer = SGD(net.parameters(), lr=0.1)</code> 中 <code>net.parameters()</code> 返回空！因为 PyTorch 只会递归查找通过 <code>nn.Module</code> 注册的子模块，不会遍历普通列表。</td>
</tr>
<tr>
<td><strong><code>.to(device)</code> 失效</strong></td>
<td>调用 <code>net.to('cuda')</code> 时，列表中的模块不会被移到 GPU，仍留在 CPU 上，导致运行时设备不匹配错误。</td>
</tr>
<tr>
<td><strong><code>.train()</code> / <code>.eval()</code> 不传播</strong></td>
<td>模型切换训练/评估模式时，子模块不会同步切换（如 Dropout、BatchNorm 行为错误）。</td>
</tr>
<tr>
<td><strong>保存/加载失败</strong></td>
<td><code>torch.save(net.state_dict())</code> 不包含子模块参数，模型无法恢复。</td>
</tr>
</tbody></table>
<p> ✅ 正确做法：</p>
<p>必须使用 <code>nn.ModuleList</code>（专为动态模块列表设计）或通过 <code>add_module</code> 注册。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正确方式 1：使用 ModuleList</span></span><br><span class="line"><span class="variable language_">self</span>.layers = nn.ModuleList([layer1, layer2])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确方式 2：动态 add_module（如 MySequential）</span></span><br><span class="line"><span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(layers):</span><br><span class="line">    <span class="variable language_">self</span>.add_module(<span class="string">f"layer_<span class="subst">{i}</span>"</span>, layer)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>📌 <strong>核心原则</strong>：所有子模块必须通过 PyTorch 的模块注册机制纳入计算图管理。</p>
</blockquote>
<hr>
<h3 id="2-实现一个“平行块”（Parallel-Block），返回两个网络的串联输出"><a href="#2-实现一个“平行块”（Parallel-Block），返回两个网络的串联输出" class="headerlink" title="2. 实现一个“平行块”（Parallel Block），返回两个网络的串联输出"></a><strong>2. 实现一个“平行块”（Parallel Block），返回两个网络的串联输出</strong></h3><blockquote>
<p>要求：输入 <code>X</code>，同时送入 <code>net1</code> 和 <code>net2</code>，将输出拼接（concatenate）后返回。</p>
</blockquote>
<p> ✅ 实现（PyTorch）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ParallelBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, net1, net2</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.net1 = net1</span><br><span class="line">        <span class="variable language_">self</span>.net2 = net2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        out1 = <span class="variable language_">self</span>.net1(X)</span><br><span class="line">        out2 = <span class="variable language_">self</span>.net2(X)</span><br><span class="line">        <span class="comment"># 沿最后一个维度拼接（假设输出是 (B, D1) 和 (B, D2)）</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat([out1, out2], dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p> 🔍 使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">net2 = nn.Sequential(nn.Linear(<span class="number">10</span>, <span class="number">3</span>), nn.ReLU())</span><br><span class="line">parallel_net = ParallelBlock(net1, net2)</span><br><span class="line"></span><br><span class="line">X = torch.randn(<span class="number">4</span>, <span class="number">10</span>)</span><br><span class="line">Y = parallel_net(X)  <span class="comment"># shape: (4, 5+3) = (4, 8)</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>💡 应用场景：多分支网络（如 Inception 模块）、特征融合、集成学习。</p>
</blockquote>
<hr>
<h3 id="3-实现一个函数，生成同一块的多个实例，并构建更大网络"><a href="#3-实现一个函数，生成同一块的多个实例，并构建更大网络" class="headerlink" title="3. 实现一个函数，生成同一块的多个实例，并构建更大网络"></a><strong>3. 实现一个函数，生成同一块的多个实例，并构建更大网络</strong></h3><blockquote>
<p>目标：避免手动复制粘贴，支持任意数量的重复块（如 ResNet 中的残差块堆叠）。</p>
</blockquote>
<p> ✅ 方法一：使用 <code>nn.ModuleList</code> + 循环</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">replicate_block</span>(<span class="params">block_fn, num_replicas</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    block_fn: 一个返回 nn.Module 的函数（避免共享参数）</span></span><br><span class="line"><span class="string">    num_replicas: 复制次数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    blocks = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas):</span><br><span class="line">        blocks.append(block_fn())  <span class="comment"># 每次调用创建新实例</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList(blocks)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例：创建 3 个独立的 Linear(10, 10) 层</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_linear</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Linear(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">replicated_layers = replicate_block(make_linear, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建前向传播（顺序执行）</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> replicated_layers:</span><br><span class="line">    x = layer(x)</span><br></pre></td></tr></table></figure>

<p> ✅ 方法二：封装成一个可复用的网络类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReplicatedNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, block_fn, num_replicas</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.blocks = nn.ModuleList([block_fn() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_replicas)])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.blocks:</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">net = ReplicatedNetwork(<span class="keyword">lambda</span>: nn.Linear(<span class="number">10</span>, <span class="number">10</span>), <span class="number">5</span>)</span><br><span class="line">output = net(torch.randn(<span class="number">3</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p> ⚠️ 重要提醒：</p>
<ul>
<li><strong>不要直接复制同一个实例</strong>（如 <code>[block] * N</code>），否则所有层共享参数！</li>
<li>必须通过<strong>函数调用</strong>或<strong>循环中新建</strong>来确保每个块是独立的。</li>
</ul>
<blockquote>
<p>✅ 正确：<code>[nn.Linear(10,10) for _ in range(N)]</code><br>❌ 错误：<code>block = nn.Linear(10,10); [block] * N</code></p>
</blockquote>
<hr>
<h3 id="✅-总结"><a href="#✅-总结" class="headerlink" title="✅ 总结"></a>✅ 总结</h3><table>
<thead>
<tr>
<th>问题</th>
<th>关键点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>1. 用 Python 列表存模块</strong></td>
<td>参数丢失、设备不一致、训练/评估模式失效 → 必须用 <code>ModuleList</code> 或 <code>add_module</code></td>
</tr>
<tr>
<td><strong>2. 平行块</strong></td>
<td>同时前向两个子网，<code>torch.cat</code> 拼接输出</td>
</tr>
<tr>
<td><strong>3. 复制多个实例</strong></td>
<td>用函数工厂 + <code>ModuleList</code>，确保参数独立</td>
</tr>
</tbody></table>
<p>这些技巧是构建复杂模型（如 Transformer、ResNet、U-Net）的基础。如果你正在实现自定义架构，建议始终使用 <code>nn.ModuleList</code> / <code>nn.ModuleDict</code> 来管理动态子模块。</p>
<h1 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h1><hr>
<p>在<strong>选择了架构并设置了超参数后，我们就进入了训练阶段</strong>。<br>此时，我们的目标是<strong>找到使损失函数最小化的模型参数值</strong>。<br>经过训练后，我们将需要使用这些参数来做出未来的预测。<br>此外，有时我们希望提取参数，以便在其他环境中复用它们，<br>将模型保存下来，以便它可以在其他软件中执行，<br>或者为了获得科学的理解而进行检查。</p>
<p>之前的介绍中，我们只依靠深度学习框架来完成训练的工作，<br>而忽略了操作参数的具体细节。<br>本节，我们将介绍以下内容：</p>
<ul>
<li>访问参数，用于调试、诊断和可视化；</li>
<li>参数初始化；</li>
<li>在不同模型组件间共享参数。</li>
</ul>
<p>(<strong>我们首先看一下具有单隐藏层的多层感知机。</strong>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))  </span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))  </span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.0970],</span><br><span class="line">        [-0.0827]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="参数访问"><a href="#参数访问" class="headerlink" title="[参数访问]"></a>[<strong>参数访问</strong>]</h2><p>我们从已有模型中访问参数。<br>当通过<code>Sequential</code>类定义模型时，<br>我们可以通过索引来访问模型的任意层。<br>这就像模型是一个列表一样，每层的参数都在其属性中。<br>如下所示，我们可以检查第二个全连接层的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(<span class="string">'weight'</span>, tensor([[ 0.3142,  0.1109, -0.1314,  0.0338, -0.3372, -0.0788,  0.1093,  0.1218]])), (<span class="string">'bias'</span>, tensor([0.2525]))])</span><br></pre></td></tr></table></figure>

<p>输出的结果告诉我们一些重要的事情：<br>首先，这个全连接层包含两个参数，分别是该层的<strong>权重和偏置</strong>。<br>两者都存储为单精度浮点数（float32）。<br>注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。  </p>
<h3 id="目标参数"><a href="#目标参数" class="headerlink" title="[目标参数]"></a>[<strong>目标参数</strong>]</h3><p>注意，每个参数都表示为参数类的一个实例。<br>要对参数执行任何操作，首先我们需要访问底层的数值。<br>有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。<br>下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，<br>提取后返回的是一个参数类实例，并进一步访问该参数的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))  </span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)  </span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">'torch.nn.parameter.Parameter'</span>&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.2525], requires_grad=True)</span><br><span class="line">tensor([0.2525])</span><br></pre></td></tr></table></figure>


<p>参数是复合的对象，包含值、梯度和额外信息。<br>这就是我们需要显式参数值的原因。<br>除了值之外，我们还可以访问每个参数的梯度。<br>在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">2</span>].weight.grad == <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure>

<h3 id="一次性访问所有参数"><a href="#一次性访问所有参数" class="headerlink" title="[一次性访问所有参数]"></a>[<strong>一次性访问所有参数</strong>]</h3><p>当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。<br>当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，<br>因为我们需要递归整个树来提取每个子块的参数。<br>下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])  </span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">'weight'</span>, torch.Size([8, 4])) (<span class="string">'bias'</span>, torch.Size([8]))</span><br><span class="line">(<span class="string">'0.weight'</span>, torch.Size([8, 4])) (<span class="string">'0.bias'</span>, torch.Size([8])) (<span class="string">'2.weight'</span>, torch.Size([1, 8])) (<span class="string">'2.bias'</span>, torch.Size([1]))</span><br></pre></td></tr></table></figure>

<p>这为我们提供了另一种访问网络参数的方式，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.state_dict()[<span class="string">'2.bias'</span>].data</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.2525])</span><br></pre></td></tr></table></figure>

<h3 id="从嵌套块收集参数"><a href="#从嵌套块收集参数" class="headerlink" title="[从嵌套块收集参数]"></a>[<strong>从嵌套块收集参数</strong>]</h3><p>让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。<br>我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():  </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),  </span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():  </span><br><span class="line">    net = nn.Sequential()  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  </span><br><span class="line">        <span class="comment"># 在这里嵌套  </span></span><br><span class="line">        net.add_module(<span class="string">f'block <span class="subst">{i}</span>'</span>, block1())  </span><br><span class="line">    <span class="keyword">return</span> net  </span><br><span class="line">  </span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))  </span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.2596],</span><br><span class="line">        [0.2596]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>[<strong>设计了网络后，我们看看它是如何工作的。</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(rgnet)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (block 0): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 1): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 2): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 3): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (1): Linear(in_features=4, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。<br>下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.3508,  0.4587,  0.4010,  0.4726, -0.2991,  0.0626, -0.3293, -0.3389])</span><br></pre></td></tr></table></figure>

<h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>知道了如何访问参数后，现在我们看看如何正确地初始化参数。<br>我们在 :numref:<code>sec_numerical_stability</code>中讨论了良好初始化的必要性。<br>深度学习框架提供默认随机初始化，<br>也允许我们创建自定义初始化方法，<br>满足我们通过其他规则实现初始化权重。</p>
<p>默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，<br>这个范围是根据输入和输出维度计算出的。<br>PyTorch的<code>nn.init</code>模块提供了多种预置初始化方法。</p>
<h3 id="内置初始化"><a href="#内置初始化" class="headerlink" title="[内置初始化]"></a>[<strong>内置初始化</strong>]</h3><p>让我们首先调用内置的初始化器。<br>下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，<br>且将偏置参数设置为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:  </span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)  </span><br><span class="line">        nn.init.zeros_(m.bias)  </span><br><span class="line">net.apply(init_normal)  </span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([-2.9892e-03, -3.6517e-03, -3.4535e-05,  1.0622e-02]), tensor(0.))</span><br></pre></td></tr></table></figure>

<p>我们还可以将所有参数初始化为给定的常数，比如初始化为1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:  </span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)  </span><br><span class="line">        nn.init.zeros_(m.bias)  </span><br><span class="line">net.apply(init_constant)  </span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 1., 1., 1.]), tensor(0.))</span><br></pre></td></tr></table></figure>

<p>我们还可以[<strong>对某些块应用不同的初始化方法</strong>]。<br>例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，<br>然后将第三个神经网络层初始化为常量值42。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_xavier</span>(<span class="params">m</span>):  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:  </span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:  </span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)  </span><br><span class="line">  </span><br><span class="line">net[<span class="number">0</span>].apply(init_xavier)  </span><br><span class="line">net[<span class="number">2</span>].apply(init_42)  </span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])  </span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([-0.0896, -0.3035, -0.4422, -0.5510])</span><br><span class="line">tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])</span><br></pre></td></tr></table></figure>

<h3 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="[自定义初始化]"></a>[<strong>自定义初始化</strong>]</h3><p>有时，深度学习框架没有提供我们需要的初始化方法。<br>在下面的例子中，我们使用以下的分布为任意权重参数<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>定义初始化方法：</p>
<p><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.803ex;" xmlns="http://www.w3.org/2000/svg" width="63.458ex" height="2.737ex" role="img" focusable="false" viewBox="0 -855 28048.5 1209.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr"><g data-mml-node="mtd"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(993.8,0)"><path data-c="223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path></g><g data-mml-node="mrow" transform="translate(2049.6,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7B" d="M477 -343L471 -349H458Q432 -349 367 -325T273 -263Q258 -245 250 -212L249 -51Q249 -27 249 12Q248 118 244 128Q243 129 243 130Q220 189 121 228Q109 232 107 235T105 250Q105 256 105 257T105 261T107 265T111 268T118 272T128 276T142 283T162 291Q224 324 243 371Q243 372 244 373Q248 384 249 469Q249 475 249 489Q249 528 249 552L250 714Q253 728 256 736T271 761T299 789T347 816T422 843Q440 849 441 849H443Q445 849 447 849T452 850T457 850H471L477 844V830Q477 820 476 817T470 811T459 807T437 801T404 785Q353 760 338 724Q333 710 333 550Q333 526 333 492T334 447Q334 393 327 368T295 318Q257 280 181 255L169 251L184 245Q318 198 332 112Q333 106 333 -49Q333 -209 338 -223Q351 -255 391 -277T469 -309Q477 -311 477 -329V-343Z"></path></g><g data-mml-node="mtable" transform="translate(583,0)"><g data-mml-node="mtr" transform="translate(0,-10)"><g data-mml-node="mtd"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mo" transform="translate(767,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1156,0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g><g data-mml-node="mo" transform="translate(1656,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(2100.7,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(3100.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mtd" transform="translate(4489.7,0)"><g data-mml-node="mtext"><path data-c="A0" d=""></path><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">可</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">能</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">性</text><path data-c="A0" d="" transform="translate(3250,0)"></path></g><g data-mml-node="mfrac" transform="translate(3500,0)"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mtext" transform="translate(4293.6,0)"><path data-c="A0" d=""></path></g><g data-mml-node="mn" transform="translate(4543.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mtd" transform="translate(10533.2,0)"><g data-mml-node="mtext"><path data-c="A0" d=""></path><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">可</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">能</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">性</text><path data-c="A0" d="" transform="translate(3250,0)"></path></g><g data-mml-node="mfrac" transform="translate(3500,0)"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mtext" transform="translate(4293.6,0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(4543.6,0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mo" transform="translate(5310.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(5699.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(6477.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(7477.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(7922.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(8700.2,0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g><g data-mml-node="mo" transform="translate(9200.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mtd" transform="translate(21122.4,0)"><g data-mml-node="mtext"><path data-c="A0" d=""></path><text data-variant="normal" transform="translate(250,0) scale(1,-1)" font-size="884px" font-family="serif">可</text><text data-variant="normal" transform="translate(1250,0) scale(1,-1)" font-size="884px" font-family="serif">能</text><text data-variant="normal" transform="translate(2250,0) scale(1,-1)" font-size="884px" font-family="serif">性</text><path data-c="A0" d="" transform="translate(3250,0)"></path></g><g data-mml-node="mfrac" transform="translate(3500,0)"><g data-mml-node="mn" transform="translate(220,394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220,-345) scale(0.707)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g></g></g></g><g data-mml-node="mo" transform="translate(25999,0) translate(0 250)"></g></g></g></g></g></g></g></svg></mjx-container><br>同样，我们实现了一个<code>my_init</code>函数来应用到<code>net</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Init"</span>, *[(name, param.shape)  </span><br><span class="line">                        <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])  </span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)  </span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span>  </span><br><span class="line">  </span><br><span class="line">net.apply(my_init)  </span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Init weight torch.Size([8, 4])</span><br><span class="line">Init weight torch.Size([1, 8])</span><br></pre></td></tr></table></figure>

<p>注意，我们始终可以直接设置参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span>  </span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span>  </span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([42.0000,  1.0000, -4.5370, -5.0581])</span><br></pre></td></tr></table></figure>


<h2 id="参数绑定"><a href="#参数绑定" class="headerlink" title="[参数绑定]"></a>[<strong>参数绑定</strong>]</h2><p>有时我们希望在多个层间共享参数：<br>我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数  </span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)  </span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),  </span><br><span class="line">                    shared, nn.ReLU(),  </span><br><span class="line">                    shared, nn.ReLU(),  </span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))  </span><br><span class="line">net(X)  </span><br><span class="line"><span class="comment"># 检查参数是否相同  </span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])  </span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span>  </span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值  </span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True, True, True, True, True])</span><br><span class="line">tensor([True, True, True, True, True, True, True, True])</span><br></pre></td></tr></table></figure>

<p>这个例子表明第三个和第五个神经网络层的参数是绑定的。<br>它们不仅值相等，而且由相同的张量表示。<br>因此，如果我们改变其中一个参数，另一个参数也会改变。<br>这里有一个问题：当参数绑定时，梯度会发生什么情况？<br>答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层<br>（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。</p>
<h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><ul>
<li>我们有几种方法可以访问、初始化和绑定模型参数。</li>
<li>我们可以使用自定义初始化方法。</li>
</ul>
<h2 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h2><ol>
<li>使用 :numref:<code>sec_model_construction</code> 中定义的<code>FancyMLP</code>模型，访问各个层的参数。</li>
<li>查看初始化模块文档以了解不同的初始化方法。</li>
<li>构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。</li>
<li>为什么共享参数是个好主意？</li>
</ol>
<h1 id="延后初始化"><a href="#延后初始化" class="headerlink" title="延后初始化"></a>延后初始化</h1><hr>
<p>:label:<code>sec_deferred_init</code>  </p>
<p>到目前为止，我们忽略了建立网络时需要做的以下这些事情：  </p>
<ul>
<li>我们定义了网络架构，但没有指定输入维度。  </li>
<li>我们添加层时没有指定前一层的输出维度。  </li>
<li>我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。</li>
</ul>
<p>毕竟，深度学习框架无法判断网络的输入维度是什么。<br>这里的诀窍是框架的<em>延后初始化</em>（defers initialization），<br>即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。  </p>
<p>在以后，当使用卷积神经网络时，<br>由于输入维度（即图像的分辨率）将影响每个后续层的维数，<br>有了该技术将更加方便。<br>现在我们在编写代码时无须知道维度是什么就可以设置参数，<br>这种能力可以大大简化定义和修改模型的任务。<br>接下来，我们将更深入地研究初始化机制。  </p>
<h2 id="实例化网络"><a href="#实例化网络" class="headerlink" title="实例化网络"></a>实例化网络</h2><p>首先，让我们实例化一个多层感知机。<br>此时，因为输入维数是未知的，所以网络不可能知道输入层权重的维数。<br>因此，框架尚未初始化任何参数，我们通过尝试访问以下参数进行确认。<br>接下来让我们将数据通过网络，最终使框架初始化参数。</p>
<p>一旦我们知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。<br>识别出第一层的形状后，框架处理第二层，依此类推，直到所有形状都已知为止。<br>注意，在这种情况下，只有第一层需要延迟初始化，但是框架仍是按顺序初始化的。<br>等到知道了所有的参数形状，框架就可以初始化参数。</p>
<h2 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h2><ul>
<li>延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。</li>
<li>我们可以通过模型传递数据，使框架最终初始化参数。</li>
</ul>
<h2 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h2><ol>
<li>如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，会发生什么？是否立即进行初始化？</li>
<li>如果指定了不匹配的维度会发生什么？</li>
<li>如果输入具有不同的维度，需要做什么？提示：查看参数绑定的相关内容。</li>
</ol>
<h1 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h1><hr>
<p>深度学习成功背后的一个因素是神经网络的灵活性：<br>我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。<br>例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。<br>有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。<br>在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。</p>
<h2 id="不带参数的层"><a href="#不带参数的层" class="headerlink" title="不带参数的层"></a>不带参数的层</h2><p>首先，我们(<strong>构造一个没有任何参数的自定义层</strong>)。<br>回忆一下在 :numref:<code>sec_model_construction</code>对块的介绍，<br>这应该看起来很眼熟。<br>下面的<code>CenteredLayer</code>类要从其输入中减去均值。<br>要构建它，我们只需继承基础层类并实现前向传播功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  </span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure>

<p>让我们向该层提供一些数据，验证它是否能按预期工作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">layer = CenteredLayer()  </span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-2., -1.,  0.,  1.,  2.])</span><br></pre></td></tr></table></figure>

<p>现在，我们可以[<strong>将层作为组件合并到更复杂的模型中</strong>]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br></pre></td></tr></table></figure>

<p>作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。<br>由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))  </span><br><span class="line">Y.mean()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(-3.7253e-09, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="带参数的层"><a href="#带参数的层" class="headerlink" title="[带参数的层]"></a>[<strong>带参数的层</strong>]</h2><p>以上我们知道了如何定义简单的层，下面我们继续定义具有参数的层，<br>这些参数可以通过训练进行调整。<br>我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。<br>比如管理访问、初始化、共享、保存和加载模型参数。<br>这样做的好处之一是：<strong>我们不需要为每个自定义层编写自定义的序列化程序</strong>。  </p>
<p>现在，让我们实现自定义版本的全连接层。<br>回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。<br>在此实现中，我们使用修正线性单元作为激活函数。<br>该层需要输入参数：<code>in_units</code>和<code>units</code>，分别表示输入数和输出数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.randn(in_units, units))  </span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.randn(units,))  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):  </span><br><span class="line">        linear = torch.matmul(X, <span class="variable language_">self</span>.weight.data) + <span class="variable language_">self</span>.bias.data  </span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure>

<p>接下来，我们实例化<code>MyLinear</code>类并访问其模型参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)  </span><br><span class="line">linear.weight</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.6312,  0.6061, -0.3756],</span><br><span class="line">        [-0.8865,  0.4480,  0.4132],</span><br><span class="line">        [-0.7441, -0.6281, -1.1005],</span><br><span class="line">        [ 0.3668,  1.1862, -0.7446],</span><br><span class="line">        [-0.2160, -0.1826,  1.3023]], requires_grad=True)</span><br></pre></td></tr></table></figure>

<p>我们可以[<strong>使用自定义层直接执行前向传播计算</strong>]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>

<p>我们还可以(<strong>使用自定义层构建模型</strong>)，就像使用内置的全连接层一样使用自定义层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))  </span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.],</span><br><span class="line">        [0.]])</span><br></pre></td></tr></table></figure>
<h2 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h2><ul>
<li>我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。  </li>
<li>在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。  </li>
<li>层可以有局部参数，这些参数可以通过内置函数创建。</li>
</ul>
<h1 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h1><hr>
<p>到目前为止，我们讨论了如何处理数据，<br>以及如何构建、训练和测试深度学习模型。<br>然而，有时我们希望保存训练的模型，<br>以备将来在各种环境中使用（比如在部署中进行预测）。<br>此外，当运行一个耗时较长的训练过程时，<br>最佳的做法是<strong>定期保存中间结果</strong>，<br>以确保<strong>在服务器电源被不小心断掉时，我们不会损失几天的计算结果</strong>。<br>因此，现在是时候学习如何<strong>加载和存储权重向量和整个模型了</strong>。</p>
<h2 id="加载和保存张量"><a href="#加载和保存张量" class="headerlink" title="(加载和保存张量)"></a>(<strong>加载和保存张量</strong>)</h2><p>对于单个张量，我们可以直接调用<code>load</code>和<code>save</code>函数分别读写它们。<br>这两个函数都要求我们提供一个名称，<code>save</code>要求将要保存的变量作为输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F  </span><br><span class="line">  </span><br><span class="line">x = torch.arange(<span class="number">4</span>)  </span><br><span class="line">torch.save(x, <span class="string">'x-file'</span>)</span><br></pre></td></tr></table></figure>

<p>我们现在可以将存储在文件中的数据读回内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x2 = torch.load(<span class="string">'x-file'</span>)  </span><br><span class="line">x2</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 1, 2, 3])</span><br></pre></td></tr></table></figure>

<p>我们可以[<strong>存储一个张量列表，然后把它们读回内存。</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)  </span><br><span class="line">torch.save([x, y],<span class="string">'x-files'</span>)  </span><br><span class="line">x2, y2 = torch.load(<span class="string">'x-files'</span>)  </span><br><span class="line">(x2, y2)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</span><br></pre></td></tr></table></figure>

<p>我们甚至可以(<strong>写入或读取从字符串映射到张量的字典</strong>)。<br>当我们要读取或写入模型中的所有权重时，这很方便。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mydict = {<span class="string">'x'</span>: x, <span class="string">'y'</span>: y}  </span><br><span class="line">torch.save(mydict, <span class="string">'mydict'</span>)  </span><br><span class="line">mydict2 = torch.load(<span class="string">'mydict'</span>)  </span><br><span class="line">mydict2</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">{<span class="string">'x'</span>: tensor([0, 1, 2, 3]), <span class="string">'y'</span>: tensor([0., 0., 0., 0.])}</span><br></pre></td></tr></table></figure>


<h2 id="加载和保存模型参数"><a href="#加载和保存模型参数" class="headerlink" title="[加载和保存模型参数]"></a>[<strong>加载和保存模型参数</strong>]</h2><p>保存单个权重向量（或其他张量）确实有用，<br>但是如果我们想保存整个模型，并在以后加载它们，<br>单独保存每个向量则会变得很麻烦。<br>毕竟，我们可能有数百个参数散布在各处。<br>因此，深度学习框架提供了内置函数来保存和加载整个网络。<br>需要注意的一个重要细节是，<strong>这将保存模型的参数而不是保存整个模型</strong>。<br>例如，如果我们有一个3层多层感知机，我们需要单独指定架构。<br>因为模型本身可以包含任意代码，所以模型本身难以序列化。<br>因此，为了恢复模型，我们需要用代码生成架构，<br>然后从磁盘加载参数。<br>让我们从熟悉的多层感知机开始尝试一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):  </span><br><span class="line">        <span class="built_in">super</span>().__init__()  </span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  </span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):  </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(F.relu(<span class="variable language_">self</span>.hidden(x)))  </span><br><span class="line">  </span><br><span class="line">net = MLP()  </span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))  </span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure>

<p>接下来，我们[<strong>将模型的参数存储在一个叫做“mlp.params”的文件中。</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">'mlp.params'</span>)</span><br></pre></td></tr></table></figure>

<p>为了恢复模型，我们[<strong>实例化了原始多层感知机模型的一个备份。</strong>]<br>这里我们不需要随机初始化模型参数，而是(<strong>直接读取文件中存储的参数。</strong>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()  </span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">'mlp.params'</span>))  </span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MLP(</span><br><span class="line">  (hidden): Linear(in_features=20, out_features=256, bias=True)</span><br><span class="line">  (output): Linear(in_features=256, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>由于两个实例具有相同的模型参数，在输入相同的<code>X</code>时，<br>两个实例的计算结果应该相同。<br>让我们来验证一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_clone = clone(X)  </span><br><span class="line">Y_clone == Y</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True, True, True, True, True, True, True, True, True],</span><br><span class="line">        [True, True, True, True, True, True, True, True, True, True]])</span><br></pre></td></tr></table></figure>

<h2 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h2><ul>
<li><code>save</code>和<code>load</code>函数可用于张量对象的文件读写。  </li>
<li>我们可以通过参数字典保存和加载网络的全部参数。  </li>
<li>保存架构必须在代码中完成，而不是在参数中完成。</li>
</ul>
<h2 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h2><ol>
<li>即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？  </li>
<li>假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？  </li>
<li>如何同时保存网络架构和参数？需要对架构加上什么限制？</li>
</ol>
<h1 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h1><hr>
<p>:label:<code>sec_use_gpu</code></p>
<p>在 :numref:<code>tab_intro_decade</code>中，<br>我们回顾了过去20年计算能力的快速增长。<br>简而言之，自2000年以来，GPU性能每十年增长1000倍。</p>
<p>本节，我们将讨论如何利用这种计算性能进行研究。<br>首先是如何使用单个GPU，然后是如何使用多个GPU和多个服务器（具有多个GPU）。</p>
<p>我们先看看如何使用单个NVIDIA GPU进行计算。<br>首先，确保至少安装了一个NVIDIA GPU。<br>然后，下载<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">NVIDIA驱动和CUDA</a><br>并按照提示设置适当的路径。<br>当这些准备工作完成，就可以使用<code>nvidia-smi</code>命令来(<strong>查看显卡信息。</strong>)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>

<p><img src="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/file-20251219163052595.png"><br>在PyTorch中，每个数组都有一个设备（device），<br>我们通常将其称为环境（context）。<br>默认情况下，所有变量和相关的计算都分配给CPU。<br>有时环境可能是GPU。<br>当我们跨多个服务器部署作业时，事情会变得更加棘手。<br>通过智能地将数组分配给环境，<br>我们可以最大限度地减少在设备之间传输数据的时间。<br>例如，当在带有GPU的服务器上训练神经网络时，<br>我们通常希望模型的参数在GPU上。</p>
<p>要运行此部分中的程序，至少需要两个GPU。<br>注意，对大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得。<br>例如可以使用AWS EC2的多GPU实例。<br>本书的其他章节大都不需要多个GPU，<br>而本节只是为了展示数据如何在不同的设备之间传递。  </p>
<h2 id="计算设备"><a href="#计算设备" class="headerlink" title="[计算设备]"></a>[<strong>计算设备</strong>]</h2><p>我们可以指定用于存储和计算的设备，如CPU和GPU。<br>默认情况下，张量是在内存中创建的，然后使用CPU计算它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  </span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  </span><br><span class="line">  </span><br><span class="line">torch.device(<span class="string">'cpu'</span>), torch.device(<span class="string">'cuda'</span>), torch.device(<span class="string">'cuda:1'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(device(<span class="built_in">type</span>=<span class="string">'cpu'</span>), device(<span class="built_in">type</span>=<span class="string">'cuda'</span>), device(<span class="built_in">type</span>=<span class="string">'cuda'</span>, index=1))</span><br></pre></td></tr></table></figure>

<p>我们可以(<strong>查询可用gpu的数量。</strong>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure>
<p>由于本人用的是笔记本吗，所以只有一个gpu</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>

<p>现在我们定义了两个方便的函数，<br>[<strong>这两个函数允许我们在不存在所需所有GPU的情况下运行代码。</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):  <span class="comment">#@save  </span></span><br><span class="line">    <span class="string">"""如果存在，则返回gpu(i)，否则返回cpu()"""</span>  </span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:  </span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f'cuda:<span class="subst">{i}</span>'</span>)  </span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">'cpu'</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():  <span class="comment">#@save  </span></span><br><span class="line">    <span class="string">"""返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""</span>  </span><br><span class="line">    devices = [torch.device(<span class="string">f'cuda:<span class="subst">{i}</span>'</span>)  </span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]  </span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">'cpu'</span>)]  </span><br><span class="line">  </span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(device(<span class="built_in">type</span>=<span class="string">'cuda'</span>, index=0),</span><br><span class="line"> device(<span class="built_in">type</span>=<span class="string">'cpu'</span>),</span><br><span class="line"> [device(<span class="built_in">type</span>=<span class="string">'cuda'</span>, index=0)])</span><br></pre></td></tr></table></figure>


<h2 id="张量与GPU"><a href="#张量与GPU" class="headerlink" title="张量与GPU"></a>张量与GPU</h2><p>我们可以[<strong>查询张量所在的设备。</strong>]<br>默认情况下，张量是在CPU上创建的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  </span><br><span class="line">x.device</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(<span class="built_in">type</span>=<span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure>

<p>需要注意的是，无论何时我们要对多个项进行操作，<br>它们都必须在同一个设备上。<br>例如，<strong>如果我们对两个张量求和，</strong><br><strong>我们需要确保两个张量都位于同一个设备上</strong>，<br>否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。  </p>
<h3 id="存储在GPU上"><a href="#存储在GPU上" class="headerlink" title="[存储在GPU上]"></a>[<strong>存储在GPU上</strong>]</h3><p>有几种方法可以在GPU上存储张量。<br>例如，我们可以在创建张量时指定存储设备。接<br>下来，我们在第一个<code>gpu</code>上创建张量变量<code>X</code>。<br>在GPU上创建的张量只消耗这个GPU的显存。<br>我们可以使用<code>nvidia-smi</code>命令查看显存使用情况。<br>一般来说，我们需要确保不创建超过GPU显存限制的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())  </span><br><span class="line">X</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure>

<p>假设我们至少有两个GPU，下面的代码将在(<strong>第二个GPU上创建一个随机张量。</strong>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))  </span><br><span class="line">Y</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.4860, 0.1285, 0.0440],</span><br><span class="line">        [0.9743, 0.4159, 0.9979]], device=<span class="string">'cuda:1'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h3><p>如果我们[<strong>要计算<code>X + Y</code>，我们需要决定在哪里执行这个操作</strong>]。<br>例如，如 :numref:<code>fig_copyto</code>所示，<br>我们可以将<code>X</code>传输到第二个GPU并在那里执行操作。<br><em>不要</em>简单地<code>X</code>加上<code>Y</code>，因为这会导致异常，<br>运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。<br>由于<code>Y</code>位于第二个GPU上，所以我们需要将<code>X</code>移到那里，<br>然后才能执行相加运算。  </p>
<p><img src="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/copyto.svg" alt="复制数据以在同一设备上执行操作"><br>:label:<code>fig_copyto</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)  </span><br><span class="line"><span class="built_in">print</span>(X)  </span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], device=<span class="string">'cuda:1'</span>)</span><br></pre></td></tr></table></figure>

<p>[<strong>现在数据在同一个GPU上（<code>Z</code>和<code>Y</code>都在），我们可以将它们相加。</strong>]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y + Z</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1.4860, 1.1285, 1.0440],</span><br><span class="line">        [1.9743, 1.4159, 1.9979]], device=<span class="string">'cuda:1'</span>)</span><br></pre></td></tr></table></figure>

<p>假设变量<code>Z</code>已经存在于第二个GPU上。<br>如果我们还是调用<code>Z.cuda(1)</code>会发生什么？<br>它将返回<code>Z</code>，而不会复制并分配新内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z.cuda(<span class="number">1</span>) <span class="keyword">is</span> Z</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure>

<h3 id="旁注"><a href="#旁注" class="headerlink" title="旁注"></a>旁注</h3><p>人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。<br>但是在<strong>设备（CPU、GPU和其他机器）之间传输数据比计算慢得多</strong>。<br>这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收），<br>然后才能继续进行更多的操作。<br>这就是为什么拷贝操作要格外小心。<br>根据经验，多个小操作比一个大操作糟糕得多。<br>此外，一次执行几个操作比代码中散布的许多单个操作要好得多。<br>如果<strong>一个设备必须等待另一个设备才能执行其他操作，</strong><br><strong>那么这样的操作可能会阻塞</strong>。<br>这有点像排队订购咖啡，而不像通过电话预先订购：<br>当客人到店的时候，咖啡已经准备好了。  </p>
<p>最后，当我们打印张量或将张量转换为NumPy格式时，<br>如果数据不在内存中，框架会首先将其复制到内存中，<br>这会导致额外的传输开销。<br>更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。  </p>
<h2 id="神经网络与GPU"><a href="#神经网络与GPU" class="headerlink" title="[神经网络与GPU]"></a>[<strong>神经网络与GPU</strong>]</h2><p>类似地，神经网络模型可以指定设备。<br>下面的代码将模型参数放在GPU上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))  </span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure>

<p>在接下来的几章中，<br>我们将看到更多关于如何在GPU上运行模型的例子，<br>因为它们将变得更加计算密集。  </p>
<p>当输入为GPU上的张量时，模型将在同一GPU上计算结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.4275],</span><br><span class="line">        [-0.4275]], device=<span class="string">'cuda:0'</span>, grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>


<p>让我们(<strong>确认模型参数存储在同一个GPU上。</strong>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.device</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(<span class="built_in">type</span>=<span class="string">'cuda'</span>, index=0)</span><br></pre></td></tr></table></figure>

<p>总之，<strong>只要所有的数据和参数都在同一个设备上，</strong><br><strong>我们就可以有效地学习模型</strong>。  </p>
<h2 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h2><ul>
<li>我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。  </li>
<li>深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。  </li>
<li><strong>不经意地移动数据可能会显著降低性能</strong>。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy <code>ndarray</code>中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。</li>
</ul>
<h2 id="练习-4"><a href="#练习-4" class="headerlink" title="练习"></a>练习</h2><ol>
<li>尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？  </li>
<li>我们应该如何在GPU上读写模型参数？  </li>
<li>测量计算1000个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="9.553ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 4222.4 688"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"></path></g><g data-mml-node="mo" transform="translate(1722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(2722.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"></path></g></g></g></svg></mjx-container>矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。  </li>
<li>测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：应该看到近乎线性的缩放。</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://cryingatnight.github.io">Yinjin Yao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://cryingatnight.github.io/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/">https://cryingatnight.github.io/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://cryingatnight.github.io" target="_blank">Yinjin Yao的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deeplearning/">deeplearning</a></div><div class="post-share"><div class="social-share" data-image="/img/lita2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机"><img class="cover" src="/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">多层感知机</div></div><div class="info-2"><div class="info-item-1">多层感知机:label:sec_mlp 隐藏层在网络中加入隐藏层我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。下面，我们以图的方式描述了多层感知机（ :numref:fig_mlp）。  .wrlcdmzfcnyn{} :label:`fig_mlp`  这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。  从线性到非线性同之前的章节一样，我们通过矩阵来表示个样本的小批量，其中每个样本具有个输入特征。对于具有个隐藏单元的单隐藏层多层感知机，用表示隐藏层...</div></div></div></a><a class="pagination-related" href="/2025/12/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="线性神经网络"><img class="cover" src="/img/lita5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">线性神经网络</div></div><div class="info-2"><div class="info-item-1">线性回归 回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。 在机器学习领域中的大多数任务通常都与预测（prediction） 有关。当我们想预测一个数值时，就会涉及到回归问题。常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、预测需求（零售销量等）。但不是所有的预测都是回归问题。 线性回归的基本元素线性回归基于几个简单的假设： 首先，假设自变量和因变量之间的关系是线性的，即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。 通常，我们使用来表示数据集中的样本数。对索引为的样本，其输入表示为，其对应的标签是。 线性模型线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子： :eqlabel:eq_price-area  :eqref:eq_price-area中的 和称为权重（weight），权重决定了每个特征对我们预测值的影响。称为*偏置（bias）、*偏移量（offs...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机"><img class="cover" src="/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-19</div><div class="info-item-2">多层感知机</div></div><div class="info-2"><div class="info-item-1">多层感知机:label:sec_mlp 隐藏层在网络中加入隐藏层我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制，使其能处理更普遍的函数关系类型。要做到这一点，最简单的方法是将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。我们可以把前层看作表示，把最后一层看作线性预测器。这种架构通常称为多层感知机（multilayer perceptron），通常缩写为MLP。下面，我们以图的方式描述了多层感知机（ :numref:fig_mlp）。  .wrlcdmzfcnyn{} :label:`fig_mlp`  这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。  从线性到非线性同之前的章节一样，我们通过矩阵来表示个样本的小批量，其中每个样本具有个输入特征。对于具有个隐藏单元的单隐藏层多层感知机，用表示隐藏层...</div></div></div></a><a class="pagination-related" href="/2025/12/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="线性神经网络"><img class="cover" src="/img/lita5.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-17</div><div class="info-item-2">线性神经网络</div></div><div class="info-2"><div class="info-item-1">线性回归 回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。 在机器学习领域中的大多数任务通常都与预测（prediction） 有关。当我们想预测一个数值时，就会涉及到回归问题。常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、预测需求（零售销量等）。但不是所有的预测都是回归问题。 线性回归的基本元素线性回归基于几个简单的假设： 首先，假设自变量和因变量之间的关系是线性的，即可以表示为中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。 通常，我们使用来表示数据集中的样本数。对索引为的样本，其输入表示为，其对应的标签是。 线性模型线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子： :eqlabel:eq_price-area  :eqref:eq_price-area中的 和称为权重（weight），权重决定了每个特征对我们预测值的影响。称为*偏置（bias）、*偏移量（offs...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/headimage.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Yinjin Yao</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">32</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cryingatnight/cryingatnight.github.io"><i class="fab fa-github"></i><span>关注</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/cryingatnight/cryingatnight.github.io" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1816192779@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B1%82%E5%92%8C%E5%9D%97"><span class="toc-text">层和块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%97"><span class="toc-text">[自定义块]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%BA%E5%BA%8F%E5%9D%97"><span class="toc-text">[顺序块]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%87%BD%E6%95%B0%E4%B8%AD%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81"><span class="toc-text">[在前向传播函数中执行代码]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-text">练习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%A6%82%E6%9E%9C%E5%B0%86-MySequential-%E4%B8%AD%E5%AD%98%E5%82%A8%E5%9D%97%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9B%B4%E6%94%B9%E4%B8%BA-Python-%E5%88%97%E8%A1%A8%EF%BC%8C%E4%BC%9A%E5%87%BA%E7%8E%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-text">1. 如果将 MySequential 中存储块的方式更改为 Python 列表，会出现什么问题？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E2%80%9C%E5%B9%B3%E8%A1%8C%E5%9D%97%E2%80%9D%EF%BC%88Parallel-Block%EF%BC%89%EF%BC%8C%E8%BF%94%E5%9B%9E%E4%B8%A4%E4%B8%AA%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%B2%E8%81%94%E8%BE%93%E5%87%BA"><span class="toc-text">2. 实现一个“平行块”（Parallel Block），返回两个网络的串联输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%EF%BC%8C%E7%94%9F%E6%88%90%E5%90%8C%E4%B8%80%E5%9D%97%E7%9A%84%E5%A4%9A%E4%B8%AA%E5%AE%9E%E4%BE%8B%EF%BC%8C%E5%B9%B6%E6%9E%84%E5%BB%BA%E6%9B%B4%E5%A4%A7%E7%BD%91%E7%BB%9C"><span class="toc-text">3. 实现一个函数，生成同一块的多个实例，并构建更大网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%9C%85-%E6%80%BB%E7%BB%93"><span class="toc-text">✅ 总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="toc-text">参数管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="toc-text">[参数访问]</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%8F%82%E6%95%B0"><span class="toc-text">[目标参数]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E6%AC%A1%E6%80%A7%E8%AE%BF%E9%97%AE%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0"><span class="toc-text">[一次性访问所有参数]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%B5%8C%E5%A5%97%E5%9D%97%E6%94%B6%E9%9B%86%E5%8F%82%E6%95%B0"><span class="toc-text">[从嵌套块收集参数]</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">参数初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">[内置初始化]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">[自定义初始化]</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="toc-text">[参数绑定]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-1"><span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BB%B6%E5%90%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">延后初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%8C%96%E7%BD%91%E7%BB%9C"><span class="toc-text">实例化网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-2"><span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-text">自定义层</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-text">不带参数的层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-text">[带参数的层]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-3"><span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="toc-text">读写文件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E5%BC%A0%E9%87%8F"><span class="toc-text">(加载和保存张量)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">[加载和保存模型参数]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-4"><span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-3"><span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GPU"><span class="toc-text">GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%AE%BE%E5%A4%87"><span class="toc-text">[计算设备]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8EGPU"><span class="toc-text">张量与GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%9C%A8GPU%E4%B8%8A"><span class="toc-text">[存储在GPU上]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6"><span class="toc-text">复制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%81%E6%B3%A8"><span class="toc-text">旁注</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8EGPU"><span class="toc-text">[神经网络与GPU]</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-5"><span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-4"><span class="toc-text">练习</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机"><img src="/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多层感知机"/></a><div class="content"><a class="title" href="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="多层感知机">多层感知机</a><time datetime="2025-12-19T09:22:47.141Z" title="更新于 2025-12-19 17:22:47">2025-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="线性神经网络"><img src="/img/lita5.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="线性神经网络"/></a><div class="content"><a class="title" href="/2025/12/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="线性神经网络">线性神经网络</a><time datetime="2025-12-19T09:20:47.907Z" title="更新于 2025-12-19 17:20:47">2025-12-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/" title="深度学习计算"><img src="/img/lita2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习计算"/></a><div class="content"><a class="title" href="/2025/12/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97/" title="深度学习计算">深度学习计算</a><time datetime="2025-12-19T09:19:10.147Z" title="更新于 2025-12-19 17:19:10">2025-12-19</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By Yinjin Yao</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div><div class="footer_custom_text">感谢阅读</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="输入以搜索内容..." type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>