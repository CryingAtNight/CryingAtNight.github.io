<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>hadoop问答小测验 | Yinjin Yao的博客</title><meta name="author" content="Yinjin Yao"><meta name="copyright" content="Yinjin Yao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="hadoop问答小测验1. hadoop组件有哪些?分别有什么功能一、核心组件由三个基础组件构成，它们是整个框架的基石：  Hadoop Distributed File System（HDFS）—— 分布式文件系统  功能： 专为存储海量数据设计的分布式文件系统，具有高容错性和高吞吐量。  将大文件分割成固定大小的 “块”（默认 128MB），分散存储在集群的多个节点上，每个块会有多个副本（默认">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop问答小测验">
<meta property="og:url" content="https://cryingatnight.github.io/2025/07/07/hadoop%E9%97%AE%E7%AD%94%E5%B0%8F%E6%B5%8B%E9%AA%8C/index.html">
<meta property="og:site_name" content="Yinjin Yao的博客">
<meta property="og:description" content="hadoop问答小测验1. hadoop组件有哪些?分别有什么功能一、核心组件由三个基础组件构成，它们是整个框架的基石：  Hadoop Distributed File System（HDFS）—— 分布式文件系统  功能： 专为存储海量数据设计的分布式文件系统，具有高容错性和高吞吐量。  将大文件分割成固定大小的 “块”（默认 128MB），分散存储在集群的多个节点上，每个块会有多个副本（默认">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cryingatnight.github.io/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg">
<meta property="article:published_time" content="2025-07-06T16:00:00.000Z">
<meta property="article:modified_time" content="2025-08-09T17:12:07.645Z">
<meta property="article:author" content="Yinjin Yao">
<meta property="article:tag" content="Hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cryingatnight.github.io/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "hadoop问答小测验",
  "url": "https://cryingatnight.github.io/2025/07/07/hadoop%E9%97%AE%E7%AD%94%E5%B0%8F%E6%B5%8B%E9%AA%8C/",
  "image": "https://cryingatnight.github.io/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg",
  "datePublished": "2025-07-06T16:00:00.000Z",
  "dateModified": "2025-08-09T17:12:07.645Z",
  "author": [
    {
      "@type": "Person",
      "name": "Yinjin Yao",
      "url": "https://cryingatnight.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/headimage.png"><link rel="canonical" href="https://cryingatnight.github.io/2025/07/07/hadoop%E9%97%AE%E7%AD%94%E5%B0%8F%E6%B5%8B%E9%AA%8C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'hadoop问答小测验',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/headimage.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-flask"></i><span> 实验室</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="http://121.199.61.105/"><i class="fa-fw fas fa-q"></i><span> 豆瓣网开发</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fuckornot.on.websim.com/"><i class="fa-fw fa fa-trophy"></i><span> 上不上AI评分系统</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fontawesome.com/icons"><i class="fa-fw fa fa-check-circle"></i><span> font-awesome v6 图标</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/headimage.png" alt="Logo"><span class="site-name">Yinjin Yao的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">hadoop问答小测验</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-flask"></i><span> 实验室</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="http://121.199.61.105/"><i class="fa-fw fas fa-q"></i><span> 豆瓣网开发</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fuckornot.on.websim.com/"><i class="fa-fw fa fa-trophy"></i><span> 上不上AI评分系统</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://fontawesome.com/icons"><i class="fa-fw fa fa-check-circle"></i><span> font-awesome v6 图标</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">hadoop问答小测验</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-06T16:00:00.000Z" title="发表于 2025-07-07 00:00:00">2025-07-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-09T17:12:07.645Z" title="更新于 2025-08-10 01:12:07">2025-08-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="hadoop问答小测验"><a href="#hadoop问答小测验" class="headerlink" title="hadoop问答小测验"></a>hadoop问答小测验</h1><h2 id="1-hadoop组件有哪些-分别有什么功能"><a href="#1-hadoop组件有哪些-分别有什么功能" class="headerlink" title="1. hadoop组件有哪些?分别有什么功能"></a>1. hadoop组件有哪些?分别有什么功能</h2><h3 id="一、核心组件"><a href="#一、核心组件" class="headerlink" title="一、核心组件"></a>一、核心组件</h3><p>由三个基础组件构成，它们是整个框架的基石：</p>
<ol>
<li><p><strong>Hadoop Distributed File System（HDFS）—— 分布式文件系统</strong></p>
<ul>
<li><strong>功能</strong>：<ul>
<li><p>专为存储海量数据设计的分布式文件系统，具有高容错性和高吞吐量。</p>
</li>
<li><p>将大文件分割成固定大小的 “块”（默认 128MB），分散存储在集群的多个节点上，每个块会有多个副本（默认 3 个），确保数据安全。<br>适合处理 “一次写入、多次读取” 的场景，不支持频繁的文件修改（更适合静态数据）。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>MapReduce—— 分布式计算框架</strong></p>
<ul>
<li><strong>功能：</strong></li>
</ul>
</li>
</ol>
<p>​		基于 “分而治之” 思想的编程模型，用于并行处理大规模数据。</p>
<ul>
<li><strong>Map 阶段</strong>：将输入数据分割成多个子任务，并行处理后生成中间结果（键值对）。</li>
<li><strong>Reduce 阶段</strong>：汇总 Map 阶段的中间结果，合并计算得到最终输出。</li>
</ul>
<ol start="3">
<li><strong>YARN（Yet Another Resource Negotiator）—— 资源管理器</strong></li>
</ol>
<ul>
<li><strong>功能 ：</strong><br>负责集群资源（CPU、内存等）的统一管理和任务调度，替代了早期 Hadoop 中的 JobTracker 和 TaskTracker。</li>
<li><strong>ResourceManager</strong>：全局资源管理器，分配资源给应用程序。</li>
<li><strong>NodeManager</strong>：每个节点上的资源管理器，监控节点资源使用情况。</li>
<li><strong>ApplicationMaster</strong>：为每个应用程序（如 MapReduce 任务）申请资源并协调任务执行。</li>
</ul>
<h3 id="二、生态系统组件"><a href="#二、生态系统组件" class="headerlink" title="二、生态系统组件"></a><strong>二、生态系统组件</strong></h3><h4 id="1-Hive——-数据仓库工具"><a href="#1-Hive——-数据仓库工具" class="headerlink" title="1. Hive—— 数据仓库工具"></a>1. <strong>Hive—— 数据仓库工具</strong></h4><ul>
<li><p>功能：</p>
<p>提供类 SQL（HQL）查询语言，将 SQL 语句转换为 MapReduce 任务，方便非程序员处理大数据。</p>
<ul>
<li>支持数据存储在 HDFS 或 HBase 中，适合离线数据分析和报表生成。</li>
<li>缺点：延迟较高，不适合实时查询。</li>
</ul>
</li>
</ul>
<h4 id="2-Sqoop——-数据迁移工具"><a href="#2-Sqoop——-数据迁移工具" class="headerlink" title="2. Sqoop—— 数据迁移工具"></a>2. <strong>Sqoop—— 数据迁移工具</strong></h4><ul>
<li><p>功能 ：</p>
<p>在关系型数据库（如 MySQL、Oracle）和 Hadoop 生态（HDFS、Hive、HBase）之间进行数据双向迁移。</p>
<ul>
<li>底层通过 MapReduce 任务实现高效并行传输，适合批量数据导入 &#x2F; 导出。</li>
</ul>
</li>
</ul>
<h4 id="3-Spark——-快速计算引擎（生态关联组件）"><a href="#3-Spark——-快速计算引擎（生态关联组件）" class="headerlink" title="3. Spark—— 快速计算引擎（生态关联组件）"></a>3. Spark—— 快速计算引擎（生态关联组件）</h4><p>功能：<br>虽然不属于 Hadoop 核心，但常与 Hadoop 配合使用，是基于内存的分布式计算引擎，比 MapReduce 快 10-100 倍。<br>支持批处理、流处理、机器学习等，兼容 HDFS、HBase 等存储系统，可通过 YARN 调度资源。</p>
<h2 id="2-分布式存储的组件是什么-有哪些进程-进程的作用是什么"><a href="#2-分布式存储的组件是什么-有哪些进程-进程的作用是什么" class="headerlink" title="2. 分布式存储的组件是什么 ?有哪些进程? 进程的作用是什么?"></a>2. 分布式存储的组件是什么 ?有哪些进程? 进程的作用是什么?</h2><p>在 Hadoop 生态中，分布式存储的核心组件是<strong>HDFS（Hadoop Distributed File System）</strong>，其架构包含多个关键进程。以下是详细介绍：</p>
<h3 id="一、HDFS-核心组件与进程"><a href="#一、HDFS-核心组件与进程" class="headerlink" title="一、HDFS 核心组件与进程"></a><strong>一、HDFS 核心组件与进程</strong></h3><p>HDFS 采用<strong>主从架构（Master-Slave）</strong>，主要由以下进程组成：</p>
<h4 id="1-NameNode（主节点进程）"><a href="#1-NameNode（主节点进程）" class="headerlink" title="1. NameNode（主节点进程）"></a>1. <strong>NameNode（主节点进程）</strong></h4><ul>
<li><p>作用：</p>
<p>作为 HDFS 的 “大脑”，管理文件系统的命名空间（文件目录结构）和客户端对文件的访问。</p>
<ul>
<li>维护文件元数据（如文件名、权限、块位置映射），但不存储实际数据。</li>
<li>处理客户端的读写请求，决定数据块的存储位置。</li>
</ul>
</li>
<li><p><strong>特点</strong>：<br>单点故障风险（HDFS 2.0 引入了 HA 高可用方案解决此问题）。</p>
</li>
</ul>
<h4 id="2-DataNode（从节点进程）"><a href="#2-DataNode（从节点进程）" class="headerlink" title="2. DataNode（从节点进程）"></a>2. <strong>DataNode（从节点进程）</strong></h4><ul>
<li><p>作用：</p>
<p>负责实际数据的存储和读写操作，分布在集群的各个节点上。</p>
<ul>
<li>将数据以 “块”（Block）形式存储在本地磁盘，默认每个块 128MB，并维护块的校验和。</li>
<li>根据 NameNode 的指令执行数据块的创建、删除和复制操作。</li>
</ul>
</li>
<li><p><strong>与客户端交互</strong>：<br>直接处理客户端的数据读写请求，不经过 NameNode（减轻 NameNode 负担）。</p>
</li>
</ul>
<h4 id="3-SecondaryNameNode（辅助名称节点进程）"><a href="#3-SecondaryNameNode（辅助名称节点进程）" class="headerlink" title="3. SecondaryNameNode（辅助名称节点进程）"></a>3. <strong>SecondaryNameNode（辅助名称节点进程）</strong></h4><ul>
<li><p>作用：</p>
<p>定期合并 NameNode 的编辑日志（edits log）和命名空间镜像（fsimage），辅助 NameNode 进行元数据管理。</p>
<ul>
<li>并非 NameNode 的热备，而是帮助减少 NameNode 重启时的恢复时间。</li>
</ul>
</li>
<li><p><strong>工作流程</strong>：<br>从 NameNode 获取最新的 fsimage 和 edits log，合并后返回给 NameNode。</p>
</li>
</ul>
<h4 id="4-JournalNode（高可用进程，HDFS-2-0-）"><a href="#4-JournalNode（高可用进程，HDFS-2-0-）" class="headerlink" title="4. JournalNode（高可用进程，HDFS 2.0+）"></a>4. <strong>JournalNode（高可用进程，HDFS 2.0+）</strong></h4><ul>
<li><p>作用：</p>
<p>在 HA（高可用）架构中，多个 JournalNode 组成 Quorum，存储 NameNode 的编辑日志（edits）。</p>
<ul>
<li>当 Active NameNode 更新元数据时，会同步写入 JournalNode 集群。</li>
<li>Standby NameNode 从 JournalNode 读取更新，保持与 Active NameNode 的状态同步。</li>
</ul>
</li>
</ul>
<h4 id="5-ZKFailoverController（高可用进程，HDFS-2-0-）"><a href="#5-ZKFailoverController（高可用进程，HDFS-2-0-）" class="headerlink" title="5. ZKFailoverController（高可用进程，HDFS 2.0+）"></a>5. <strong>ZKFailoverController（高可用进程，HDFS 2.0+）</strong></h4><ul>
<li><p>作用：</p>
<p>基于 ZooKeeper 实现 NameNode 的自动故障转移（Failover）。</p>
<ul>
<li>监控 NameNode 状态，当 Active NameNode 失效时，自动将 Standby NameNode 切换为 Active 状态。</li>
</ul>
</li>
</ul>
<h3 id="二、HDFS-高可用（HA）架构进程"><a href="#二、HDFS-高可用（HA）架构进程" class="headerlink" title="二、HDFS 高可用（HA）架构进程"></a><strong>二、HDFS 高可用（HA）架构进程</strong></h3><p>为避免单点故障，HDFS 2.0 引入了 HA 方案，涉及以下额外进程：</p>
<h4 id="1-Active-NameNode"><a href="#1-Active-NameNode" class="headerlink" title="1. Active NameNode"></a>1. <strong>Active NameNode</strong></h4><ul>
<li><strong>作用</strong>：<br>处理客户端请求的主 NameNode，负责实时更新元数据。</li>
</ul>
<h4 id="2-Standby-NameNode"><a href="#2-Standby-NameNode" class="headerlink" title="2. Standby NameNode"></a>2. <strong>Standby NameNode</strong></h4><ul>
<li><strong>作用</strong>：<br>作为 Active NameNode 的热备，通过 JournalNode 同步元数据，确保在 Active 故障时能快速接管。</li>
</ul>
<h4 id="3-ZooKeeper-集群"><a href="#3-ZooKeeper-集群" class="headerlink" title="3. ZooKeeper 集群"></a>3. <strong>ZooKeeper 集群</strong></h4><ul>
<li><strong>作用</strong>：<br>协调 Active 和 Standby NameNode 的状态，保证同一时刻只有一个 Active NameNode。</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>HDFS 的核心进程通过分工协作，实现了大规模数据的可靠存储和高效访问：</p>
<ul>
<li><strong>NameNode</strong>：管理元数据，协调读写请求。</li>
<li><strong>DataNode</strong>：存储实际数据块，执行数据读写。</li>
<li><strong>SecondaryNameNode</strong>：辅助元数据管理。</li>
<li><strong>JournalNode&#x2F;ZKFC</strong>：保障高可用性。</li>
</ul>
<p>理解这些进程的角色和交互，是优化 HDFS 性能和排查故障的关键。</p>
<h2 id="3-资源调度使用什么组件-该组件有哪些进程-每个进程的作用是什么"><a href="#3-资源调度使用什么组件-该组件有哪些进程-每个进程的作用是什么" class="headerlink" title="3. 资源调度使用什么组件? 该组件有哪些进程?每个进程的作用是什么?"></a>3. 资源调度使用什么组件? 该组件有哪些进程?每个进程的作用是什么?</h2><p>在 Hadoop 生态中，资源调度主要由 <strong>YARN（Yet Another Resource Negotiator）</strong> 组件负责。它是 Hadoop 2.0 引入的资源管理系统，替代了早期 MapReduce 的 JobTracker&#x2F;Tracker 架构，实现了计算资源的统一管理和多框架共享。</p>
<h3 id="一、YARN-的核心组件与进程"><a href="#一、YARN-的核心组件与进程" class="headerlink" title="一、YARN 的核心组件与进程"></a><strong>一、YARN 的核心组件与进程</strong></h3><p>YARN 采用 <strong>主从架构（Master-Slave）</strong>，主要由以下进程组成：</p>
<h4 id="1-ResourceManager（RM，主节点进程）"><a href="#1-ResourceManager（RM，主节点进程）" class="headerlink" title="1. ResourceManager（RM，主节点进程）"></a>1. <strong>ResourceManager（RM，主节点进程）</strong></h4><ul>
<li><p>作用：</p>
<p>作为全局资源管理器，负责整个集群的资源分配和调度。</p>
<ul>
<li>接收来自各个 <strong>ApplicationMaster</strong> 的资源请求。</li>
<li>根据集群资源使用情况和预设的调度策略（如容量调度、公平调度），将资源分配给应用程序。</li>
<li>监控 NodeManager 的健康状态，处理节点故障。</li>
</ul>
</li>
<li><p>内部组件：</p>
<ul>
<li><strong>Scheduler</strong>：纯调度器，负责分配资源，不参与应用程序监控和状态管理。</li>
<li><strong>ApplicationsManager</strong>：管理所有应用程序的生命周期，接收新应用的提交请求。</li>
</ul>
</li>
</ul>
<h4 id="2-NodeManager（NM，从节点进程）"><a href="#2-NodeManager（NM，从节点进程）" class="headerlink" title="2. NodeManager（NM，从节点进程）"></a>2. <strong>NodeManager（NM，从节点进程）</strong></h4><ul>
<li><p>作用：</p>
<p>运行在每个计算节点上，负责本地资源管理和容器监控。</p>
<ul>
<li>启动和监控 <strong>Container</strong>（资源隔离单元，封装 CPU、内存等资源）。</li>
<li>向 ResourceManager 汇报节点资源使用情况和容器状态。</li>
<li>根据 ResourceManager 的指令，启动或停止 Container。</li>
</ul>
</li>
</ul>
<h4 id="3-ApplicationMaster（AM，每个应用独有的进程）"><a href="#3-ApplicationMaster（AM，每个应用独有的进程）" class="headerlink" title="3. ApplicationMaster（AM，每个应用独有的进程）"></a>3. <strong>ApplicationMaster（AM，每个应用独有的进程）</strong></h4><ul>
<li><p>作用：</p>
<p>每个应用程序（如 MapReduce 作业、Spark 任务）启动时都会创建一个 AM，负责该应用的资源申请和任务协调。</p>
<ul>
<li>向 ResourceManager 注册并请求资源（Container）。</li>
<li>与 NodeManager 通信，启动和监控任务容器。</li>
<li>跟踪应用程序的执行状态，向 ResourceManager 汇报进度。</li>
</ul>
</li>
<li><p><strong>示例</strong>：<br>MapReduce 作业的 AM 负责将任务拆分为 Map 和 Reduce 任务，并分配到合适的节点执行。</p>
</li>
</ul>
<h4 id="4-Container（资源容器）"><a href="#4-Container（资源容器）" class="headerlink" title="4. Container（资源容器）"></a>4. <strong>Container（资源容器）</strong></h4><ul>
<li><p>作用：</p>
<p>是 YARN 中的资源隔离单元，由 NodeManager 管理，运行具体的任务（如 Map&#x2F;Reduce 任务）。</p>
<ul>
<li>每个 Container 有严格的资源限制（CPU、内存），通过 Linux cgroups 实现隔离。</li>
<li>任务完成后，Container 被 NodeManager 回收。</li>
</ul>
</li>
</ul>
<h3 id="二、YARN-的调度器类型"><a href="#二、YARN-的调度器类型" class="headerlink" title="二、YARN 的调度器类型"></a><strong>二、YARN 的调度器类型</strong></h3><p>ResourceManager 的 <strong>Scheduler</strong> 支持多种调度策略，常见的有：</p>
<h4 id="1-容量调度器（Capacity-Scheduler）"><a href="#1-容量调度器（Capacity-Scheduler）" class="headerlink" title="1. 容量调度器（Capacity Scheduler）"></a>1. <strong>容量调度器（Capacity Scheduler）</strong></h4><ul>
<li><strong>特点</strong>：<br>将集群资源划分为多个队列，每个队列有固定的资源上限，支持层级队列和资源共享。</li>
<li><strong>应用场景</strong>：<br>多租户环境，不同部门 &#x2F; 用户共享集群，保证每个队列至少获得一定资源。</li>
</ul>
<h4 id="2-公平调度器（Fair-Scheduler）"><a href="#2-公平调度器（Fair-Scheduler）" class="headerlink" title="2. 公平调度器（Fair Scheduler）"></a>2. <strong>公平调度器（Fair Scheduler）</strong></h4><ul>
<li><strong>特点</strong>：<br>动态平衡资源分配，优先为资源不足的应用分配资源，实现 “公平” 共享。</li>
<li><strong>应用场景</strong>：<br>交互式作业与批处理作业共存的场景，避免大作业长时间占用资源。</li>
</ul>
<h4 id="3-层次调度器（Hierarchical-Scheduler）"><a href="#3-层次调度器（Hierarchical-Scheduler）" class="headerlink" title="3. 层次调度器（Hierarchical Scheduler）"></a>3. <strong>层次调度器（Hierarchical Scheduler）</strong></h4><ul>
<li><strong>特点</strong>：<br>结合容量调度和公平调度的优势，支持多级队列和资源继承。</li>
</ul>
<h3 id="三、YARN-高可用（HA）架构进程"><a href="#三、YARN-高可用（HA）架构进程" class="headerlink" title="三、YARN 高可用（HA）架构进程"></a><strong>三、YARN 高可用（HA）架构进程</strong></h3><p>为避免单点故障，YARN 支持 ResourceManager 的 HA 部署，涉及以下进程：</p>
<h4 id="1-Active-ResourceManager"><a href="#1-Active-ResourceManager" class="headerlink" title="1. Active ResourceManager"></a>1. <strong>Active ResourceManager</strong></h4><ul>
<li><strong>作用</strong>：<br>处理客户端请求的主 RM，负责实时资源调度。</li>
</ul>
<h4 id="2-Standby-ResourceManager"><a href="#2-Standby-ResourceManager" class="headerlink" title="2. Standby ResourceManager"></a>2. <strong>Standby ResourceManager</strong></h4><ul>
<li><strong>作用</strong>：<br>作为热备，同步 Active RM 的状态，在主节点故障时自动接管。</li>
</ul>
<h4 id="3-ZooKeeper-集群-1"><a href="#3-ZooKeeper-集群-1" class="headerlink" title="3. ZooKeeper 集群"></a>3. <strong>ZooKeeper 集群</strong></h4><ul>
<li><strong>作用</strong>：<br>协调 Active 和 Standby RM 的状态，确保同一时刻只有一个 Active RM。</li>
</ul>
<h4 id="4-ZKFC（ZooKeeper-Failover-Controller）"><a href="#4-ZKFC（ZooKeeper-Failover-Controller）" class="headerlink" title="4. ZKFC（ZooKeeper Failover Controller）"></a>4. <strong>ZKFC（ZooKeeper Failover Controller）</strong></h4><ul>
<li><strong>作用</strong>：<br>监控 RM 状态，基于 ZooKeeper 实现自动故障转移。</li>
</ul>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>YARN 通过分离资源管理和作业调度，实现了集群资源的高效利用和多框架协作：</p>
<ul>
<li><strong>ResourceManager</strong>：全局资源调度，负责资源分配。</li>
<li><strong>NodeManager</strong>：节点资源管理，运行容器。</li>
<li><strong>ApplicationMaster</strong>：应用程序的资源协调者。</li>
<li><strong>Container</strong>：资源隔离的基本单位。</li>
</ul>
<p>理解 YARN 的架构和调度机制，有助于优化集群资源利用率，提升大数据作业的执行效率。</p>
<h2 id="4-分布式计算分为几个步骤-shuffle有几个步骤"><a href="#4-分布式计算分为几个步骤-shuffle有几个步骤" class="headerlink" title="4. 分布式计算分为几个步骤? shuffle有几个步骤?"></a>4. 分布式计算分为几个步骤? shuffle有几个步骤?</h2><h3 id="一、分布式计算的典型步骤（以-MapReduce-为例）"><a href="#一、分布式计算的典型步骤（以-MapReduce-为例）" class="headerlink" title="一、分布式计算的典型步骤（以 MapReduce 为例）"></a>一、分布式计算的典型步骤（以 MapReduce 为例）</h3><p>分布式计算框架（如 MapReduce、Spark）通常将大规模数据处理任务分解为多个阶段，以实现并行化和容错性。以下是典型的分布式计算流程：</p>
<h4 id="1-数据分片（Data-Splitting）"><a href="#1-数据分片（Data-Splitting）" class="headerlink" title="1. 数据分片（Data Splitting）"></a>1. <strong>数据分片（Data Splitting）</strong></h4><ul>
<li>将输入数据划分为多个小块（如 HDFS 中的 Block），每个分片由一个 Map 任务处理。</li>
<li><strong>目的</strong>：实现数据的并行处理。</li>
</ul>
<h4 id="2-Map-阶段"><a href="#2-Map-阶段" class="headerlink" title="2. Map 阶段"></a>2. <strong>Map 阶段</strong></h4><ul>
<li>每个 Map 任务处理一个数据分片，执行用户定义的映射函数（如提取、转换数据）。</li>
<li><strong>输出</strong>：中间键值对（Key-Value）。</li>
</ul>
<h4 id="3-Shuffle-阶段"><a href="#3-Shuffle-阶段" class="headerlink" title="3. Shuffle 阶段"></a>3. <strong>Shuffle 阶段</strong></h4><ul>
<li><strong>核心作用</strong>：将 Map 输出的中间结果按 Key 分组，并分发到对应的 Reduce 任务。</li>
<li><strong>关键步骤</strong>（见下文详细拆解）。</li>
</ul>
<h4 id="4-Reduce-阶段"><a href="#4-Reduce-阶段" class="headerlink" title="4. Reduce 阶段"></a>4. <strong>Reduce 阶段</strong></h4><ul>
<li>每个 Reduce 任务处理一组具有相同 Key 的数据，执行聚合或计算逻辑。</li>
<li><strong>输出</strong>：最终结果数据。</li>
</ul>
<h4 id="5-结果合并与存储"><a href="#5-结果合并与存储" class="headerlink" title="5. 结果合并与存储"></a>5. <strong>结果合并与存储</strong></h4><ul>
<li>将所有 Reduce 任务的输出合并（可选），存储到分布式文件系统（如 HDFS）或其他存储介质。</li>
</ul>
<h3 id="二、Shuffle-阶段的详细步骤（以-MapReduce-为例）"><a href="#二、Shuffle-阶段的详细步骤（以-MapReduce-为例）" class="headerlink" title="二、Shuffle 阶段的详细步骤（以 MapReduce 为例）"></a>二、Shuffle 阶段的详细步骤（以 MapReduce 为例）</h3><p>Shuffle 是分布式计算中最关键且复杂的环节，负责数据的重组和分发。在 MapReduce 中，Shuffle 分为 Map 端和 Reduce 端两部分，共包含 6 个核心步骤：</p>
<h4 id="Map-端的-Shuffle-步骤"><a href="#Map-端的-Shuffle-步骤" class="headerlink" title="Map 端的 Shuffle 步骤"></a><strong>Map 端的 Shuffle 步骤</strong></h4><ol>
<li><strong>Map 输出写入内存缓冲区</strong><ul>
<li>Map 任务的输出首先写入内存中的环形缓冲区（默认 100MB）。</li>
<li>缓冲区同时存储数据和元数据（记录 Key 的分区信息、偏移量等）。</li>
</ul>
</li>
<li><strong>溢写（Spill）到磁盘</strong><ul>
<li>当缓冲区使用率达到阈值（默认 80%）时，触发溢写线程将数据写入本地磁盘。</li>
<li><strong>排序</strong>：在溢写前，数据按 Key 的哈希值进行分区（Partition），并对每个分区内的 Key 进行排序（如快速排序）。</li>
<li><strong>合并</strong>：如果配置了 Combiner（本地聚合函数），会在溢写前对相同 Key 的值进行预聚合。</li>
</ul>
</li>
<li><strong>磁盘文件合并</strong><ul>
<li>多个溢写文件（Spill File）最终合并为一个更大的分区排序文件（Partitioned &amp; Sorted File）。</li>
<li>合并过程中会保留分区和排序属性，减少 Reduce 端的处理负担。</li>
</ul>
</li>
</ol>
<h4 id="Reduce-端的-Shuffle-步骤"><a href="#Reduce-端的-Shuffle-步骤" class="headerlink" title="Reduce 端的 Shuffle 步骤"></a><strong>Reduce 端的 Shuffle 步骤</strong></h4><ol>
<li><strong>远程数据拉取（Fetch）</strong><ul>
<li>Reduce 任务启动后，通过 HTTP 从所有 Map 任务的输出文件中拉取属于自己分区的数据。</li>
<li><strong>并行拉取</strong>：多个线程同时从不同 Map 节点获取数据，提高效率。</li>
</ul>
</li>
<li><strong>内存与磁盘数据合并</strong><ul>
<li>拉取的数据首先存入内存缓冲区，当达到阈值时溢写到磁盘。</li>
<li>多次溢写会生成多个磁盘文件，最终合并为一个大文件。</li>
<li><strong>归并排序</strong>：合并过程中对数据按 Key 进行全局排序（如果需要）。</li>
</ul>
</li>
<li><strong>Reduce 处理</strong><ul>
<li>排序后的 Key-Value 按组（相同 Key 的所有 Value）输入到 Reduce 函数中进行计算。</li>
<li>最终结果输出到 HDFS 或其他存储系统。</li>
</ul>
</li>
</ol>
<h2 id="5-hdfs文件系统的读取数据流程是什么"><a href="#5-hdfs文件系统的读取数据流程是什么" class="headerlink" title="5. hdfs文件系统的读取数据流程是什么?"></a>5. hdfs文件系统的读取数据流程是什么?</h2><p>HDFS（Hadoop 分布式文件系统）的读取数据流程主要涉及客户端、NameNode（元数据节点）和 DataNode（数据节点）三个核心组件，流程可分为以下关键步骤：</p>
<h3 id="1-客户端发起读取请求"><a href="#1-客户端发起读取请求" class="headerlink" title="1. 客户端发起读取请求"></a><strong>1. 客户端发起读取请求</strong></h3><p>客户端通过调用 HDFS 提供的 API（如<code>FileSystem.open(path)</code>）发起文件读取请求。此时，客户端会先与 HDFS 的<code>DistributedFileSystem</code>（分布式文件系统抽象类）交互，告知需要读取的文件路径。</p>
<h3 id="2-获取文件元数据（与-NameNode-交互）"><a href="#2-获取文件元数据（与-NameNode-交互）" class="headerlink" title="2. 获取文件元数据（与 NameNode 交互）"></a><strong>2. 获取文件元数据（与 NameNode 交互）</strong></h3><p><code>DistributedFileSystem</code>收到请求后，会向<strong>NameNode</strong>发送请求，获取目标文件的元数据。元数据包括：</p>
<ul>
<li>文件被分割成的<strong>数据块（Block）列表</strong>（文件按固定大小分割，默认块大小为 128MB 或 256MB）；</li>
<li>每个数据块的<strong>副本存储位置</strong>（即该块的多个副本分别存放在哪些 DataNode 上）。</li>
</ul>
<p>NameNode 仅存储元数据，不存储实际数据，因此会直接返回上述信息。</p>
<h3 id="3-客户端初始化输入流"><a href="#3-客户端初始化输入流" class="headerlink" title="3. 客户端初始化输入流"></a><strong>3. 客户端初始化输入流</strong></h3><p><code>DistributedFileSystem</code>收到 NameNode 返回的元数据后，会为客户端返回一个<code>FSDataInputStream</code>（文件系统数据输入流）对象。该对象内部封装了<code>DFSInputStream</code>（分布式文件系统输入流），负责后续与 DataNode 的实际数据交互。</p>
<h3 id="4-客户端读取数据（与-DataNode-交互）"><a href="#4-客户端读取数据（与-DataNode-交互）" class="headerlink" title="4. 客户端读取数据（与 DataNode 交互）"></a><strong>4. 客户端读取数据（与 DataNode 交互）</strong></h3><p><code>DFSInputStream</code>根据 NameNode 返回的元数据，按顺序读取文件的各个数据块，具体过程如下：</p>
<ul>
<li><strong>选择最优 DataNode</strong>：对于每个数据块，<code>DFSInputStream</code>会优先选择距离客户端最近的 DataNode（例如同一机架的节点，减少跨机架网络传输），若该节点不可用则自动切换到其他副本所在的 DataNode。</li>
<li><strong>直接读取数据</strong>：客户端通过<code>DFSInputStream</code>与选中的 DataNode 建立连接，直接读取该块的数据（数据传输不经过 NameNode，减轻 NameNode 负担）。</li>
<li><strong>顺序读取所有块</strong>：一个块读取完成后，<code>DFSInputStream</code>会关闭与当前 DataNode 的连接，自动连接下一个块的最优 DataNode，继续读取，直到所有块均读取完毕。</li>
</ul>
<h3 id="5-容错处理"><a href="#5-容错处理" class="headerlink" title="5. 容错处理"></a><strong>5. 容错处理</strong></h3><p>在读取过程中，若某个 DataNode 出现故障（如网络中断、节点宕机），<code>DFSInputStream</code>会自动处理容错：</p>
<ul>
<li>立即关闭与故障 DataNode 的连接；</li>
<li>从该块的其他副本所在的 DataNode 重新读取数据；</li>
<li>记录故障节点，后续读取时避免使用。</li>
</ul>
<h3 id="6-读取完成，关闭流"><a href="#6-读取完成，关闭流" class="headerlink" title="6. 读取完成，关闭流"></a><strong>6. 读取完成，关闭流</strong></h3><p>当客户端读取完所有数据块（拼接为完整文件）后，调用<code>close()</code>方法关闭<code>FSDataInputStream</code>，结束本次读取流程。</p>
<h3 id="核心特点总结"><a href="#核心特点总结" class="headerlink" title="核心特点总结"></a><strong>核心特点总结</strong></h3><ul>
<li><strong>元数据与数据分离</strong>：NameNode 仅提供元数据（块位置），不参与实际数据传输，数据直接在客户端与 DataNode 之间传输，减轻 NameNode 压力。</li>
<li><strong>副本利用</strong>：通过多副本存储，保证了数据读取的可靠性（某节点故障时可切换到其他副本）。</li>
<li><strong>就近读取</strong>：优先选择距离客户端最近的 DataNode（如同一机架），减少网络开销，提升效率。</li>
</ul>
<p>简言之，HDFS 读取流程可概括为：<strong>客户端请求元数据→NameNode 返回块位置→客户端直接从 DataNode 按块读取→容错处理→完成读取</strong>。</p>
<h2 id="6-hdfs文件系统的写文件流程是什么"><a href="#6-hdfs文件系统的写文件流程是什么" class="headerlink" title="6. hdfs文件系统的写文件流程是什么?"></a>6. hdfs文件系统的写文件流程是什么?</h2><p>HDFS（Hadoop 分布式文件系统）的写文件流程同样涉及客户端、NameNode 和 DataNode，核心是通过<strong>元数据管理</strong>和<strong>副本机制</strong>保证数据的可靠性和一致性，具体流程如下：</p>
<h3 id="1-客户端发起写请求"><a href="#1-客户端发起写请求" class="headerlink" title="1. 客户端发起写请求"></a><strong>1. 客户端发起写请求</strong></h3><p>客户端通过 HDFS API（如<code>FileSystem.create(path)</code>）发起文件写入请求，首先与<code>DistributedFileSystem</code>（分布式文件系统抽象类）交互，告知需要写入的文件路径。</p>
<h3 id="2-向-NameNode-请求创建文件"><a href="#2-向-NameNode-请求创建文件" class="headerlink" title="2. 向 NameNode 请求创建文件"></a><strong>2. 向 NameNode 请求创建文件</strong></h3><p><code>DistributedFileSystem</code>向<strong>NameNode</strong>发送 “创建文件” 请求，NameNode 会进行一系列校验：</p>
<ul>
<li>检查文件路径是否已存在（避免覆盖）；</li>
<li>检查客户端是否有写权限；</li>
<li>检查父目录是否存在。</li>
</ul>
<p>若校验通过，NameNode 会在元数据中记录该文件的创建状态（标记为 “正在写入”），并返回 “允许创建” 的响应；若失败（如文件已存在），则返回错误信息。</p>
<h3 id="3-客户端初始化输出流并获取-DataNode-列表"><a href="#3-客户端初始化输出流并获取-DataNode-列表" class="headerlink" title="3. 客户端初始化输出流并获取 DataNode 列表"></a><strong>3. 客户端初始化输出流并获取 DataNode 列表</strong></h3><p><code>DistributedFileSystem</code>收到允许创建的响应后，为客户端返回<code>FSDataOutputStream</code>（文件系统数据输出流）对象，其内部封装了<code>DFSOutputStream</code>（分布式输出流），负责后续数据传输和副本管理。</p>
<p>此时，客户端通过<code>DFSOutputStream</code>向 NameNode 请求<strong>第一个数据块（Block）的存储位置</strong>。NameNode 根据副本放置策略（默认 3 副本），分配 3 个 DataNode 节点（例如：1 个本地机架节点、1 个同机架其他节点、1 个异机架节点，平衡可靠性与网络开销），并将这 3 个 DataNode 的地址返回给客户端。</p>
<h3 id="4-建立数据传输管道（Pipeline）"><a href="#4-建立数据传输管道（Pipeline）" class="headerlink" title="4. 建立数据传输管道（Pipeline）"></a><strong>4. 建立数据传输管道（Pipeline）</strong></h3><p>客户端的<code>DFSOutputStream</code>根据 NameNode 返回的 DataNode 列表，建立<strong>数据传输管道</strong>：</p>
<ul>
<li>客户端先与第一个 DataNode（“管道首节点”）建立 TCP 连接；</li>
<li>第一个 DataNode 再与第二个 DataNode 建立连接；</li>
<li>第二个 DataNode 与第三个 DataNode 建立连接；<br>最终形成 “客户端→DataNode1→DataNode2→DataNode3” 的链式管道，用于数据的逐级复制（保证副本一致性）。</li>
</ul>
<h3 id="5-客户端写入数据（分块-分数据包）"><a href="#5-客户端写入数据（分块-分数据包）" class="headerlink" title="5. 客户端写入数据（分块 + 分数据包）"></a><strong>5. 客户端写入数据（分块 + 分数据包）</strong></h3><p>客户端开始写入数据，过程如下：</p>
<ul>
<li><strong>数据分块</strong>：文件按固定大小（默认 128MB）分割为数据块（Block），逐个写入；</li>
<li><strong>数据包（Packet）传输</strong>：每个数据块被拆分为更小的数据包（默认 64KB），客户端先将数据包写入本地缓冲区，再通过管道发送给第一个 DataNode；</li>
<li><strong>副本复制</strong>：第一个 DataNode 接收数据包后，立即转发给第二个 DataNode，第二个再转发给第三个，实现副本的同步复制；</li>
<li><strong>确认机制</strong>：每个 DataNode 接收数据包并写入本地后，会向 “上游” 返回确认（ACK）。最终，第三个 DataNode 的确认会逐级传回客户端，确保数据包已被所有副本节点接收。</li>
</ul>
<h3 id="6-数据块写入完成，请求新块"><a href="#6-数据块写入完成，请求新块" class="headerlink" title="6. 数据块写入完成，请求新块"></a><strong>6. 数据块写入完成，请求新块</strong></h3><p>当一个数据块的所有数据包均写入完成（且收到所有副本的确认），<code>DFSOutputStream</code>会向 NameNode 报告 “当前块写入完成”，并请求分配<strong>下一个数据块的 DataNode 列表</strong>。重复步骤 4-5，直到所有数据块均写入完成。</p>
<h3 id="7-写入完成，关闭流并确认"><a href="#7-写入完成，关闭流并确认" class="headerlink" title="7. 写入完成，关闭流并确认"></a><strong>7. 写入完成，关闭流并确认</strong></h3><p>客户端写完所有数据后，调用<code>close()</code>方法关闭<code>FSDataOutputStream</code>：</p>
<ul>
<li><code>DFSOutputStream</code>会将缓冲区中剩余的数据包全部发送，并等待最后一批确认；</li>
<li>通知 NameNode “文件写入完成”，NameNode 更新元数据（记录文件的所有块信息、副本位置等），并将文件标记为 “已完成” 状态（此时文件可被读取）。</li>
</ul>
<h3 id="容错处理（关键机制）"><a href="#容错处理（关键机制）" class="headerlink" title="容错处理（关键机制）"></a><strong>容错处理（关键机制）</strong></h3><p>写入过程中若某个 DataNode 故障（如网络中断、节点宕机），<code>DFSOutputStream</code>会自动处理：</p>
<ul>
<li>立即关闭当前管道，标记故障节点；</li>
<li>通知 NameNode “该节点故障”，NameNode 会后续调度其他节点补充副本（维持默认 3 副本）；</li>
<li>客户端重新与剩余的健康 DataNode 建立新管道（例如：若 DataNode2 故障，新管道为 “客户端→DataNode1→DataNode3”），并重发未确认的数据包；</li>
<li>确保最终所有数据块的副本数量符合配置（默认 3 个）。</li>
</ul>
<h3 id="核心特点总结-1"><a href="#核心特点总结-1" class="headerlink" title="核心特点总结"></a><strong>核心特点总结</strong></h3><ul>
<li><strong>管道复制</strong>：数据通过链式管道逐级复制到副本节点，保证副本一致性；</li>
<li><strong>确认机制</strong>：每个数据包需经所有副本节点确认后才算完成，确保数据可靠；</li>
<li><strong>机架感知</strong>：副本分配考虑机架位置（同机架 + 异机架），平衡可靠性（避免单机架故障）与网络效率（减少跨机架传输）；</li>
<li><strong>NameNode 轻量参与</strong>：仅负责元数据管理和块分配，不参与实际数据传输，降低负担。</li>
</ul>
<p>简言之，HDFS 写流程可概括为：<strong>客户端请求创建→NameNode 分配节点→建立管道复制副本→分块分包写入→完成后确认元数据</strong>，通过容错机制确保数据可靠存储。</p>
<h2 id="7-hdfs文件系统-namenode的更新流程是什么"><a href="#7-hdfs文件系统-namenode的更新流程是什么" class="headerlink" title="7. hdfs文件系统 namenode的更新流程是什么?"></a>7. hdfs文件系统 namenode的更新流程是什么?</h2><p>HDFS 中 NameNode 的核心职责是管理元数据（如文件目录结构、数据块与文件的映射、块的副本位置等），其元数据的更新流程与客户端操作（如创建文件、写入数据、删除文件、重命名等）紧密绑定，同时依赖<strong>持久化机制</strong>（保证元数据不丢失）和<strong>一致性维护</strong>（确保元数据准确）。以下是 NameNode 元数据更新的核心流程：</p>
<h3 id="一、元数据的存储形式"><a href="#一、元数据的存储形式" class="headerlink" title="一、元数据的存储形式"></a><strong>一、元数据的存储形式</strong></h3><p>NameNode 的元数据分为两种形态，更新流程围绕这两种形态展开：</p>
<ol>
<li><p><strong>内存元数据（In-Memory Metadata）</strong>：实时维护的元数据快照，包含所有文件、目录、块的最新状态（如文件是否存在、块的副本位置、块的大小等），供客户端实时查询和操作，性能极高。</p>
</li>
<li><p>持久化存储</p>
<p>：</p>
<ul>
<li><strong>FsImage</strong>：元数据的 “快照文件”，定期将内存中的元数据序列化到磁盘，是元数据的持久化基线。</li>
<li><strong>EditLog</strong>：“操作日志文件”，记录所有对元数据的修改操作（如创建文件、删除块等），是内存元数据的 “变更记录”，确保元数据更新可追溯。</li>
</ul>
</li>
</ol>
<h3 id="二、元数据更新的核心流程（以-“文件写入”-为例）"><a href="#二、元数据更新的核心流程（以-“文件写入”-为例）" class="headerlink" title="二、元数据更新的核心流程（以 “文件写入” 为例）"></a><strong>二、元数据更新的核心流程（以 “文件写入” 为例）</strong></h3><p>客户端的任何操作（创建、写入、删除等）都会触发 NameNode 的元数据更新，以最典型的 “文件写入” 为例，流程如下：</p>
<h4 id="1-客户端发起操作请求"><a href="#1-客户端发起操作请求" class="headerlink" title="1. 客户端发起操作请求"></a><strong>1. 客户端发起操作请求</strong></h4><p>客户端通过 API（如<code>create</code>、<code>append</code>等）发起对 HDFS 的修改操作（如创建文件、写入数据块），请求首先到达<code>DistributedFileSystem</code>，再转发给 NameNode。</p>
<h4 id="2-NameNode-验证操作合法性"><a href="#2-NameNode-验证操作合法性" class="headerlink" title="2. NameNode 验证操作合法性"></a><strong>2. NameNode 验证操作合法性</strong></h4><p>NameNode 收到请求后，先进行合法性校验：</p>
<ul>
<li>权限校验（客户端是否有操作权限）；</li>
<li>状态校验（如创建文件时检查路径是否已存在，写入块时检查文件是否处于 “可写入” 状态）；</li>
<li>资源校验（如分配块时检查是否有足够的 DataNode 存储空间）。</li>
</ul>
<p>若校验失败，直接返回错误；若通过，进入下一步。</p>
<h4 id="3-记录操作到-EditLog（预写日志，Write-Ahead-Log）"><a href="#3-记录操作到-EditLog（预写日志，Write-Ahead-Log）" class="headerlink" title="3. 记录操作到 EditLog（预写日志，Write-Ahead Log）"></a><strong>3. 记录操作到 EditLog（预写日志，Write-Ahead Log）</strong></h4><p>为保证元数据更新的可靠性，NameNode 采用 <strong>“先写日志，再更新内存”</strong> 的机制（Write-Ahead Logging，WAL）：</p>
<ul>
<li>将当前操作（如 “创建文件”“分配块 1 到 DataNode A、B、C”）序列化后写入<strong>EditLog</strong>（默认存储在磁盘的<code>dfs.namenode.edits.dir</code>目录）。</li>
<li>只有当 EditLog 写入成功（确保操作被持久化），才会执行后续的内存元数据更新。</li>
</ul>
<p><em>注：EditLog 是顺序写入的，性能极高，且支持多目录备份（本地磁盘 + 远程存储），防止单点故障导致日志丢失。</em></p>
<h4 id="4-更新内存元数据"><a href="#4-更新内存元数据" class="headerlink" title="4. 更新内存元数据"></a><strong>4. 更新内存元数据</strong></h4><p>EditLog 写入成功后，NameNode 立即更新<strong>内存中的元数据</strong>：</p>
<ul>
<li>例如，创建文件时，内存中新增文件条目，标记为 “正在写入” 状态；</li>
<li>写入数据块时，为文件添加新的块映射（记录块 ID 与文件的关联），并记录块的副本位置（DataNode 列表）；</li>
<li>当文件写入完成时，将文件状态从 “正在写入” 改为 “已完成”。</li>
</ul>
<p>内存元数据的更新是实时的，确保后续客户端查询能获取最新状态。</p>
<h3 id="核心特点总结-2"><a href="#核心特点总结-2" class="headerlink" title="核心特点总结"></a><strong>核心特点总结</strong></h3><ol>
<li><strong>可靠性</strong>：依赖 EditLog 的预写机制（先写日志再更新内存），确保任何元数据修改都被持久化，避免宕机丢失。</li>
<li><strong>高效性</strong>：内存元数据实时更新，保证客户端操作的低延迟；EditLog 顺序写入、FsImage 定期合并，平衡性能与持久化开销。</li>
<li><strong>一致性</strong>：通过严格的操作校验和日志记录，确保元数据的状态与实际文件系统一致（如文件要么存在要么不存在，不会出现中间态）。</li>
</ol>
<p>简言之，NameNode 的元数据更新流程可概括为：<strong>客户端请求→校验→记录 EditLog→更新内存元数据→定期合并日志与镜像→故障时重放日志恢复</strong>。</p>
<h2 id="8-整个mapreduce阶段经历几次排序-分别在哪些阶段"><a href="#8-整个mapreduce阶段经历几次排序-分别在哪些阶段" class="headerlink" title="8. 整个mapreduce阶段经历几次排序?分别在哪些阶段?"></a>8. 整个mapreduce阶段经历几次排序?分别在哪些阶段?</h2><p>在 MapReduce 的整个流程中，排序是核心机制之一，共经历<strong>三次排序</strong>，分别发生在以下阶段：</p>
<ol>
<li><strong>Map 阶段的溢写（Spill）排序</strong><br>当 Map 任务的环形缓冲区使用率达到阈值（默认 80%）时，数据会被溢写到磁盘。溢写前会对缓冲区中的数据进行<strong>快速排序</strong>，排序依据是键（Key）和分区号（Partition）136。排序后的数据按分区聚集，每个分区内的键值对按 Key 升序排列。这一步确保每个溢写文件在分区内是有序的。</li>
<li><strong>Map 阶段的合并（Merge）排序</strong><br>Map 任务完成后，磁盘上可能存在多个溢写文件。此时会对这些文件进行<strong>归并排序</strong>，将同一分区的多个溢写文件合并为一个大文件。合并过程中，数据会按 Key 再次排序，最终生成一个分区有序的输出文件。</li>
<li><strong>Reduce 阶段的输入合并排序</strong><br>Reduce 任务拉取所有 Map 任务的输出数据后，会将同一分区的多个文件进行<strong>归并排序</strong>。这一步将分散的有序数据合并成一个全局有序的文件，确保输入到 Reducer 的数据按 Key 有序排列。合并过程可能涉及内存到磁盘或磁盘到磁盘的多次合并，但最终结果是一个完整的有序数据集。</li>
</ol>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><ul>
<li><strong>排序的必要性</strong>：排序是 MapReduce 框架的默认行为，目的是将相同 Key 的数据聚集到同一个 Reducer 处理，确保数据处理的正确性和高效性。</li>
<li><strong>排序算法</strong>：Map 阶段的溢写使用快速排序，而合并阶段（包括 Map 和 Reduce）使用归并排序，这两种算法的结合保证了大规模数据处理的性能。</li>
<li><strong>输出排序</strong>：默认情况下，Reducer 的输出会按 Key 再次排序，但这属于框架自动完成的输出处理，通常不计入 MapReduce 阶段的排序次数。</li>
</ul>
<h2 id="9-hive中内部表和外部表有什么区别-分区表和分桶表有什么区别"><a href="#9-hive中内部表和外部表有什么区别-分区表和分桶表有什么区别" class="headerlink" title="9. hive中内部表和外部表有什么区别? 分区表和分桶表有什么区别?"></a>9. hive中内部表和外部表有什么区别? 分区表和分桶表有什么区别?</h2><p>在 Hive 中，内部表与外部表、分区表与分桶表是两组重要的概念，它们的区别如下：</p>
<h3 id="一、内部表（Managed-Table）-vs-外部表（External-Table）"><a href="#一、内部表（Managed-Table）-vs-外部表（External-Table）" class="headerlink" title="一、内部表（Managed Table） vs 外部表（External Table）"></a><strong>一、内部表（Managed Table） vs 外部表（External Table）</strong></h3><h4 id="1-数据管理方式"><a href="#1-数据管理方式" class="headerlink" title="1. 数据管理方式"></a><strong>1. 数据管理方式</strong></h4><ul>
<li><strong>内部表</strong><br>Hive 完全管理数据的生命周期，包括数据的存储路径、创建、删除等。当删除内部表时，数据文件也会被永久删除。</li>
<li><strong>外部表</strong><br>数据存储在 Hive 元数据指定的路径（如 HDFS），Hive 仅管理元数据（表结构）。删除外部表时，数据文件不会被删除，仅删除元数据。</li>
</ul>
<h4 id="2-创建语法差异"><a href="#2-创建语法差异" class="headerlink" title="2. 创建语法差异"></a><strong>2. 创建语法差异</strong></h4><ul>
<li><p>内部表</p>
<p>sql</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> internal_table (id <span class="type">INT</span>, name STRING);</span><br></pre></td></tr></table></figure>
</li>
<li><p>外部表</p>
<p>sql</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> external_table (id <span class="type">INT</span>, name STRING)</span><br><span class="line">LOCATION <span class="string">&#x27;/path/to/data&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-使用场景"><a href="#3-使用场景" class="headerlink" title="3. 使用场景"></a><strong>3. 使用场景</strong></h4><ul>
<li><strong>内部表</strong>：适合临时数据或测试数据，数据由 Hive 完全掌控。</li>
<li><strong>外部表</strong>：适合共享数据（如多个系统共同访问同一 HDFS 路径）或避免误删重要数据。</li>
</ul>
<h3 id="二、分区表（Partitioned-Table）-vs-分桶表（Bucketed-Table）"><a href="#二、分区表（Partitioned-Table）-vs-分桶表（Bucketed-Table）" class="headerlink" title="二、分区表（Partitioned Table） vs 分桶表（Bucketed Table）"></a><strong>二、分区表（Partitioned Table） vs 分桶表（Bucketed Table）</strong></h3><h4 id="1-数据组织方式"><a href="#1-数据组织方式" class="headerlink" title="1. 数据组织方式"></a><strong>1. 数据组织方式</strong></h4><ul>
<li><strong>分区表</strong><br>按指定字段（如日期、地区）将数据划分为多个子目录，查询时可通过分区过滤快速定位数据。<br><strong>示例</strong>：按日期分区的表，数据存储在 <code>/table/date=20230101/</code>、<code>/table/date=20230102/</code> 等目录。</li>
<li><strong>分桶表</strong><br>按指定字段的哈希值将数据分散到多个文件（桶）中，用于提高数据抽样和 JOIN 效率。<br><strong>示例</strong>：按用户 ID 分桶的表，数据被哈希到 <code>000000_0</code>、<code>000001_0</code> 等文件。</li>
</ul>
<h4 id="2-创建语法差异-1"><a href="#2-创建语法差异-1" class="headerlink" title="2. 创建语法差异"></a><strong>2. 创建语法差异</strong></h4><ul>
<li><p>分区表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> partitioned_table (id <span class="type">INT</span>, name STRING)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (dt STRING);</span><br></pre></td></tr></table></figure>
</li>
<li><p>分桶表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE TABLE</span> bucketed_table (id <span class="type">INT</span>, name STRING)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (id) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-使用场景-1"><a href="#3-使用场景-1" class="headerlink" title="3. 使用场景"></a><strong>3. 使用场景</strong></h4><ul>
<li><strong>分区表</strong>：适合过滤条件频繁涉及分区字段（如按日期查询）的场景，可大幅减少数据扫描范围。</li>
<li><strong>分桶表</strong>：适合 JOIN 操作（桶数相同且分桶字段一致时可直接匹配）或数据抽样（如 <code>TABLESAMPLE</code> 语句）。</li>
</ul>
<h3 id="三、核心区别总结"><a href="#三、核心区别总结" class="headerlink" title="三、核心区别总结"></a><strong>三、核心区别总结</strong></h3><table>
<thead>
<tr>
<th><strong>对比维度</strong></th>
<th><strong>内部表 vs 外部表</strong></th>
<th><strong>分区表 vs 分桶表</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>数据管理</strong></td>
<td>内部表管理数据，外部表仅管理元数据</td>
<td>均不影响数据生命周期</td>
</tr>
<tr>
<td><strong>数据组织形式</strong></td>
<td>无特殊组织</td>
<td>分区按字段创建子目录，分桶按哈希分文件</td>
</tr>
<tr>
<td><strong>主要目的</strong></td>
<td>控制数据所有权和生命周期</td>
<td>优化查询性能（分区过滤、分桶 JOIN）</td>
</tr>
<tr>
<td><strong>删除操作影响</strong></td>
<td>内部表删除数据，外部表保留数据</td>
<td>均不删除数据</td>
</tr>
</tbody></table>
<h3 id="四、最佳实践建议"><a href="#四、最佳实践建议" class="headerlink" title="四、最佳实践建议"></a><strong>四、最佳实践建议</strong></h3><ol>
<li><strong>内部表 &#x2F; 外部表选择</strong><ul>
<li>若数据仅由 Hive 使用且需要自动管理，用内部表。</li>
<li>若数据需被其他系统共享或保护，用外部表。</li>
</ul>
</li>
<li><strong>分区表 &#x2F; 分桶表选择</strong><ul>
<li>过滤条件常涉及某字段（如日期）时，优先分区。</li>
<li>JOIN 操作频繁或需数据抽样时，考虑分桶。</li>
<li>复杂场景可组合使用（如分区 + 分桶）。</li>
</ul>
</li>
</ol>
<h2 id="10-数据倾斜是什么-如何规避和处理"><a href="#10-数据倾斜是什么-如何规避和处理" class="headerlink" title="10. 数据倾斜是什么? 如何规避和处理?"></a>10. 数据倾斜是什么? 如何规避和处理?</h2><p>在大数据处理场景中，<strong>数据倾斜</strong>是指由于数据分布极端不均匀，导致部分计算节点承担了远超其他节点的数据量或计算压力，进而拖慢整个任务执行效率（甚至导致节点内存溢出、任务失败）的现象。</p>
<h3 id="数据倾斜的表现与影响"><a href="#数据倾斜的表现与影响" class="headerlink" title="数据倾斜的表现与影响"></a><strong>数据倾斜的表现与影响</strong></h3><ul>
<li><strong>表现</strong>：任务进度长时间停留在 99%（少数节点未完成）、部分节点日志频繁报内存警告（OOM）、不同节点处理的数据量差异悬殊（如某节点处理 1000 万条数据，其他节点仅处理 1 万条）。</li>
<li><strong>影响</strong>：任务执行时间大幅延长、资源利用率极低（多数节点空闲）、极端情况导致任务失败。</li>
</ul>
<h3 id="数据倾斜的常见场景"><a href="#数据倾斜的常见场景" class="headerlink" title="数据倾斜的常见场景"></a><strong>数据倾斜的常见场景</strong></h3><p>数据倾斜通常发生在需要<strong>shuffle 操作</strong>的环节（如<code>group by</code>、<code>join</code>、<code>distinct</code>等），因为 shuffle 会根据 key 的哈希值分配数据到不同节点，若某个 key 的数量异常多（“大 key”），则会导致对应节点负载过高。常见场景包括：</p>
<ul>
<li><code>group by key</code>时，某类 key 的数量占比超过 90%；</li>
<li><code>join</code>操作中，某张表的某个关联 key 包含数百万条数据，而其他 key 仅数千条；</li>
<li><code>distinct</code>去重时，某个值的重复次数极高（如统计用户行为时，“未知用户 ID” 的记录量极大）。</li>
</ul>
<h3 id="数据倾斜的规避方法（事前预防）"><a href="#数据倾斜的规避方法（事前预防）" class="headerlink" title="数据倾斜的规避方法（事前预防）"></a><strong>数据倾斜的规避方法（事前预防）</strong></h3><p>规避的核心是<strong>减少 “大 key” 的产生</strong>，从数据源头或计算逻辑上避免极端不均匀的分布。</p>
<h4 id="1-数据预处理：过滤-清洗无效数据"><a href="#1-数据预处理：过滤-清洗无效数据" class="headerlink" title="1. 数据预处理：过滤 &#x2F; 清洗无效数据"></a>1. <strong>数据预处理：过滤 &#x2F; 清洗无效数据</strong></h4><ul>
<li><p>提前过滤掉无意义的 “脏数据”（如空值、默认值、异常值）。例如：用户行为日志中，“未知用户 ID（如 - 1）” 的记录可能占比极高，可提前过滤或单独标记处理。</p>
</li>
<li><p>示例：在</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">join</span><br></pre></td></tr></table></figure>

<p>前，先过滤掉关联 key 为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">null</span><br></pre></td></tr></table></figure>

<p>或异常值的数据：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 过滤无效key，避免参与shuffle</span></span><br><span class="line"><span class="keyword">SELECT</span> a.id, b.name </span><br><span class="line"><span class="keyword">FROM</span> table_a a</span><br><span class="line"><span class="keyword">JOIN</span> table_b b </span><br><span class="line"><span class="keyword">ON</span> a.id <span class="operator">=</span> b.id </span><br><span class="line"><span class="keyword">WHERE</span> a.id <span class="keyword">IS</span> <span class="keyword">NOT NULL</span> <span class="keyword">AND</span> a.id <span class="operator">!=</span> <span class="number">-1</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-拆分大-key：将集中的-key-分散化"><a href="#2-拆分大-key：将集中的-key-分散化" class="headerlink" title="2. 拆分大 key：将集中的 key 分散化"></a>2. <strong>拆分大 key：将集中的 key 分散化</strong></h4><ul>
<li>若某类 key 天然存在（如热门商品 ID），可通过 “拆分” 将其转化为多个子 key，分散到不同节点。例如：将大 key <code>key001</code>拆分为<code>key001_0</code>、<code>key001_1</code>、…、<code>key001_n</code>，每个子 key 的数据量均匀。</li>
<li>适用场景：已知某 key 是大 key（如业务上已知 “热门商品 ID”），可在数据写入时提前拆分，或在计算前通过规则拆分。</li>
</ul>
<h4 id="3-选择合适的计算逻辑：避免不必要的-shuffle"><a href="#3-选择合适的计算逻辑：避免不必要的-shuffle" class="headerlink" title="3. 选择合适的计算逻辑：避免不必要的 shuffle"></a>3. <strong>选择合适的计算逻辑：避免不必要的 shuffle</strong></h4><ul>
<li><p>小表广播（Broadcast Join）：若</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">join</span><br></pre></td></tr></table></figure>

<p>的两张表中，一张是小表（数据量 &lt;广播阈值，如 10GB），可将小表广播到所有节点，避免 shuffle（大表与本地小表直接关联）。适用于 “大表 join 小表” 场景（如维度表与事实表 join）。</p>
<ul>
<li><p>Spark 中可通过</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcast()</span><br></pre></td></tr></table></figure>

<p>函数实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> broadcast</span><br><span class="line">df = big_table.join(broadcast(small_table), on=<span class="string">&quot;key&quot;</span>, how=<span class="string">&quot;left&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>避免全局<code>distinct</code>：如需统计 “用户总数”，若直接用<code>count(distinct user_id)</code>，可能因某类用户 ID 重复过多导致倾斜，可改为<code>group by user_id</code>后<code>count</code>，分散计算压力。</p>
</li>
</ul>
<h3 id="数据倾斜的处理方法（事中解决）"><a href="#数据倾斜的处理方法（事中解决）" class="headerlink" title="数据倾斜的处理方法（事中解决）"></a><strong>数据倾斜的处理方法（事中解决）</strong></h3><p>当倾斜已发生时，核心是 <strong>“拆分大 key，分散负载”</strong> 或 <strong>“单独处理大 key”</strong>，以下是具体策略：</p>
<h4 id="1-大-key-拆分（加盐法）"><a href="#1-大-key-拆分（加盐法）" class="headerlink" title="1. 大 key 拆分（加盐法）"></a>1. <strong>大 key 拆分（加盐法）</strong></h4><p>对导致倾斜的大 key，通过添加随机前缀（“加盐”）将其拆分为多个子 key，分散到不同节点处理，最后合并结果。<br><strong>示例</strong>：<code>join</code>时某 key <code>K</code>有 100 万条数据，其他 key 仅 1 万条：</p>
<ul>
<li><strong>步骤 1</strong>：对左表的<code>K</code>添加随机前缀（如<code>K_0</code>、<code>K_1</code>、…、<code>K_9</code>），拆分为 10 个子 key（数据量各 10 万）；</li>
<li><strong>步骤 2</strong>：对右表的<code>K</code>复制 10 份，分别添加相同前缀（<code>K_0</code>到<code>K_9</code>）；</li>
<li><strong>步骤 3</strong>：两表按加盐后的 key 进行<code>join</code>，此时原大 key 的 100 万数据被分散到 10 个节点处理；</li>
<li><strong>步骤 4</strong>：去掉前缀，合并结果。</li>
</ul>
<h4 id="2-调整-shuffle-分区数"><a href="#2-调整-shuffle-分区数" class="headerlink" title="2. 调整 shuffle 分区数"></a>2. <strong>调整 shuffle 分区数</strong></h4><p>默认的 shuffle 分区数（如 Spark 默认 200，Hive 默认 100）可能导致分区数不足，加剧倾斜。可通过增加分区数让数据更均匀分布：</p>
<ul>
<li>Spark：<code>spark.sql.shuffle.partitions = 1000</code>（根据数据量调整，通常每分区 100MB~1GB）；</li>
<li>Hive：<code>set mapreduce.job.reduces = 1000</code>（reduce 数即分区数）。</li>
</ul>
<h4 id="3-框架级参数优化"><a href="#3-框架级参数优化" class="headerlink" title="3. 框架级参数优化"></a>3. <strong>框架级参数优化</strong></h4><p>主流大数据框架提供了自动处理倾斜的参数，可直接开启：</p>
<ul>
<li>Spark：<ul>
<li><code>spark.sql.adaptive.enabled = true</code>（自适应执行，动态调整分区数）；</li>
<li><code>spark.sql.adaptive.skewJoin.enabled = true</code>（自动检测并处理 join 倾斜，拆分大 key）。</li>
</ul>
</li>
<li>Hive：<ul>
<li><code>set hive.optimize.skewjoin = true</code>（开启倾斜 join 优化）；</li>
<li><code>set hive.skewjoin.key = 100000</code>（定义 “大 key” 阈值，超过该值则视为倾斜）。</li>
</ul>
</li>
</ul>
<h4 id="4-过滤或单独处理大-key"><a href="#4-过滤或单独处理大-key" class="headerlink" title="4. 过滤或单独处理大 key"></a>4. <strong>过滤或单独处理大 key</strong></h4><p>若大 key 的数据无实际业务意义（如测试数据、异常值），可直接过滤；若必须保留，可单独提取大 key 数据，用 “小表广播” 或 “Map 端 join” 单独处理，再与其他结果合并。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>数据倾斜的本质是 <strong>“数据分布不均匀”</strong>，核心解决思路是：</p>
<ol>
<li><strong>事前规避</strong>：通过数据清洗、逻辑优化减少大 key 产生；</li>
<li><strong>事中处理</strong>：通过拆分大 key、调整分区、框架参数优化等分散负载。</li>
</ol>
<p>实际处理中需结合具体业务场景（如数据特点、计算逻辑）和框架特性选择合适方案，最终目标是让数据在节点间分布尽可能均匀，提升整体计算效率。</p>
<h2 id="11-如何使用hdfs文件系统上传或下载数据-关闭hdfs文件系统安全模式"><a href="#11-如何使用hdfs文件系统上传或下载数据-关闭hdfs文件系统安全模式" class="headerlink" title="11. 如何使用hdfs文件系统上传或下载数据? 关闭hdfs文件系统安全模式"></a>11. 如何使用hdfs文件系统上传或下载数据? 关闭hdfs文件系统安全模式</h2><p>HDFS（Hadoop Distributed File System）是 Hadoop 生态系统的核心组件，用于存储和管理大规模数据集。下面介绍如何使用 HDFS 命令行工具上传 &#x2F; 下载数据，以及如何关闭安全模式。</p>
<h3 id="一、HDFS-数据上传与下载"><a href="#一、HDFS-数据上传与下载" class="headerlink" title="一、HDFS 数据上传与下载"></a><strong>一、HDFS 数据上传与下载</strong></h3><h4 id="1-上传数据到-HDFS（从本地到-HDFS）"><a href="#1-上传数据到-HDFS（从本地到-HDFS）" class="headerlink" title="1. 上传数据到 HDFS（从本地到 HDFS）"></a><strong>1. 上传数据到 HDFS（从本地到 HDFS）</strong></h4><p>使用<code>hdfs dfs -put</code>或<code>hdfs dfs -copyFromLocal</code>命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法：hdfs dfs -put &lt;本地路径&gt; &lt;HDFS目标路径&gt;</span></span><br><span class="line">hdfs dfs -put /home/user/local_data.txt /user/hdfs/data/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或使用copyFromLocal（功能相同）</span></span><br><span class="line">hdfs dfs -copyFromLocal /home/user/local_data.txt /user/hdfs/data/</span><br></pre></td></tr></table></figure>

<p><strong>参数说明</strong>：</p>
<ul>
<li><code>-f</code>：强制覆盖 HDFS 上已存在的文件（若目标路径已存在同名文件）。</li>
<li><code>-p</code>：保留原始文件的权限、时间戳等属性。</li>
</ul>
<p><strong>示例</strong>：上传目录及其所有子文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put /home/user/local_dir /user/hdfs/</span><br></pre></td></tr></table></figure>

<h4 id="2-从-HDFS-下载数据（从-HDFS-到本地）"><a href="#2-从-HDFS-下载数据（从-HDFS-到本地）" class="headerlink" title="2. 从 HDFS 下载数据（从 HDFS 到本地）"></a><strong>2. 从 HDFS 下载数据（从 HDFS 到本地）</strong></h4><p>使用<code>hdfs dfs -get</code>或<code>hdfs dfs -copyToLocal</code>命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语法：hdfs dfs -get &lt;HDFS路径&gt; &lt;本地目标路径&gt;</span></span><br><span class="line">hdfs dfs -get /user/hdfs/data.txt /home/user/downloads/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或使用copyToLocal（功能相同）</span></span><br><span class="line">hdfs dfs -get /user/hdfs/data.txt /home/user/downloads/</span><br></pre></td></tr></table></figure>

<p><strong>参数说明</strong>：</p>
<ul>
<li><code>-f</code>：强制覆盖本地已存在的文件。</li>
<li><code>-ignoreCrc</code>：忽略 CRC 校验（若文件损坏但仍需下载）。</li>
</ul>
<p><strong>示例</strong>：下载整个目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -get /user/hdfs/dir /home/user/downloads/</span><br></pre></td></tr></table></figure>

<h4 id="3-其他常用-HDFS-操作命令"><a href="#3-其他常用-HDFS-操作命令" class="headerlink" title="3. 其他常用 HDFS 操作命令"></a><strong>3. 其他常用 HDFS 操作命令</strong></h4><ul>
<li><p>查看 HDFS 文件列表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">ls</span> /user/hdfs/</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看文件内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">cat</span> /user/hdfs/data.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建目录:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /user/hdfs/new_dir</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除文件 &#x2F; 目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">rm</span> -r /user/hdfs/to_delete</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="二、关闭-HDFS-安全模式"><a href="#二、关闭-HDFS-安全模式" class="headerlink" title="二、关闭 HDFS 安全模式"></a><strong>二、关闭 HDFS 安全模式</strong></h3><h4 id="1-安全模式简介"><a href="#1-安全模式简介" class="headerlink" title="1. 安全模式简介"></a><strong>1. 安全模式简介</strong></h4><p>安全模式是 HDFS 的一种保护机制，用于在系统启动、故障恢复或维护期间防止数据被修改。在此模式下：</p>
<ul>
<li>客户端只能读取数据，不能写入或删除文件；</li>
<li>NameNode 不会执行块的复制或删除操作；</li>
<li>当集群中超过 99.9% 的块满足最小副本数要求时，安全模式可手动关闭。</li>
</ul>
<h4 id="2-查看当前安全模式状态"><a href="#2-查看当前安全模式状态" class="headerlink" title="2. 查看当前安全模式状态"></a><strong>2. 查看当前安全模式状态</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode get</span><br><span class="line"><span class="comment"># 输出示例：Safe mode is OFF（关闭）或Safe mode is ON（开启）</span></span><br></pre></td></tr></table></figure>

<h4 id="3-手动关闭安全模式（需管理员权限）"><a href="#3-手动关闭安全模式（需管理员权限）" class="headerlink" title="3. 手动关闭安全模式（需管理员权限）"></a><strong>3. 手动关闭安全模式（需管理员权限）</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode leave</span><br></pre></td></tr></table></figure>

<p><strong>执行流程</strong>：</p>
<ol>
<li>NameNode 检查集群中数据块的状态，确保足够的副本存在；</li>
<li>若满足条件（通常 99.9% 以上的块有足够副本），安全模式自动退出；</li>
<li>若不满足条件，需等待数据块复制完成或手动修复（如添加 DataNode）。</li>
</ol>
<h4 id="4-强制退出安全模式（谨慎使用）"><a href="#4-强制退出安全模式（谨慎使用）" class="headerlink" title="4. 强制退出安全模式（谨慎使用）"></a><strong>4. 强制退出安全模式（谨慎使用）</strong></h4><p>若集群处于异常状态（如数据块丢失），但仍需强制关闭安全模式，可使用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode forceExit</span><br></pre></td></tr></table></figure>

<p><strong>风险提示</strong>：强制退出可能导致数据不一致或丢失，仅在紧急情况下使用。</p>
<h4 id="5-进入安全模式（用于维护）"><a href="#5-进入安全模式（用于维护）" class="headerlink" title="5. 进入安全模式（用于维护）"></a><strong>5. 进入安全模式（用于维护）</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode enter</span><br></pre></td></tr></table></figure>

<h3 id="三、常见问题与解决方案"><a href="#三、常见问题与解决方案" class="headerlink" title="三、常见问题与解决方案"></a><strong>三、常见问题与解决方案</strong></h3><h4 id="1-上传文件时提示-“安全模式开启”"><a href="#1-上传文件时提示-“安全模式开启”" class="headerlink" title="1. 上传文件时提示 “安全模式开启”"></a><strong>1. 上传文件时提示 “安全模式开启”</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cannot create file/user/hdfs/data.txt. Name node is in safe mode.</span><br></pre></td></tr></table></figure>

<p><strong>解决</strong>：执行<code>hdfs dfsadmin -safemode leave</code>关闭安全模式。</p>
<h4 id="2-安全模式无法自动退出"><a href="#2-安全模式无法自动退出" class="headerlink" title="2. 安全模式无法自动退出"></a><strong>2. 安全模式无法自动退出</strong></h4><p>可能原因：</p>
<ul>
<li>数据块副本不足，NameNode 持续等待复制完成；</li>
<li>DataNode 与 NameNode 通信异常（检查网络和日志）。<br><strong>解决</strong>：</li>
</ul>
<ol>
<li><p>检查数据块状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /</span><br></pre></td></tr></table></figure>
</li>
<li><p>若有丢失块，尝试修复或重新上传数据；</p>
</li>
<li><p>确认所有 DataNode 正常运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -report</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="3-权限问题"><a href="#3-权限问题" class="headerlink" title="3. 权限问题"></a><strong>3. 权限问题</strong></h4><p>若提示<code>Permission denied</code>，需确保当前用户有操作权限，或使用<code>hdfs dfs -chmod</code>修改权限：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">chmod</span> 775 /user/hdfs/dir  <span class="comment"># 赋予读写执行权限</span></span><br></pre></td></tr></table></figure>

<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><strong>上传数据</strong>：<code>hdfs dfs -put &lt;本地路径&gt; &lt;HDFS路径&gt;</code></li>
<li><strong>下载数据</strong>：<code>hdfs dfs -get &lt;HDFS路径&gt; &lt;本地路径&gt;</code></li>
<li><strong>关闭安全模式</strong>：<code>hdfs dfsadmin -safemode leave</code></li>
<li><strong>维护建议</strong>：定期检查 HDFS 健康状态（<code>hdfs dfsadmin -report</code>），确保数据块副本数达标，避免频繁进入安全模式。</li>
</ul>
<h2 id="12-order-by-和sort-by-的区别和使用场景"><a href="#12-order-by-和sort-by-的区别和使用场景" class="headerlink" title="12. order by 和sort by 的区别和使用场景?"></a>12. order by 和sort by 的区别和使用场景?</h2><p><code>ORDER BY</code> 和 <code>SORT BY</code> 是在数据处理和查询中用于排序的操作，但它们在功能、应用场景和实现机制上存在一些区别，下面为你详细介绍：</p>
<h3 id="1-SQL-中的-ORDER-BY"><a href="#1-SQL-中的-ORDER-BY" class="headerlink" title="1. SQL 中的 ORDER BY"></a>1. SQL 中的 <code>ORDER BY</code></h3><ul>
<li><p><strong>功能</strong>：对查询结果进行全局排序，确保最终输出的结果集完全有序。</p>
</li>
<li><p><strong>应用场景</strong>：需要最终结果严格有序的场景，如生成报表、导出排序后的数据等。</p>
</li>
<li><p>特点：</p>
<ul>
<li>对整个结果集进行排序，开销较大。</li>
<li>在分布式环境中，需要将所有数据收集到一个节点上进行排序，可能导致数据倾斜和性能问题。</li>
</ul>
</li>
<li><p>示例:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> salary <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-SQL-中的-SORT-BY（部分数据库支持）"><a href="#2-SQL-中的-SORT-BY（部分数据库支持）" class="headerlink" title="2. SQL 中的 SORT BY（部分数据库支持）"></a>2. SQL 中的 <code>SORT BY</code>（部分数据库支持）</h3><ul>
<li><p><strong>功能</strong>：在某些数据库（如 Hive）中，<code>SORT BY</code> 用于在每个分区内进行排序，而非全局排序。</p>
</li>
<li><p><strong>应用场景</strong>：在分布式处理中，不需要全局排序，但需要每个分区内部有序的场景。</p>
</li>
<li><p>特点：</p>
<ul>
<li>每个分区独立排序，不同分区之间的顺序无法保证。</li>
<li>性能优于 <code>ORDER BY</code>，因为不需要数据集中。</li>
</ul>
</li>
<li><p>示例</p>
<p>（Hive 语法）：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">DISTRIBUTE <span class="keyword">BY</span> department</span><br><span class="line">SORT <span class="keyword">BY</span> salary <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>

<p>这个查询会将数据按部门分区，每个部门内按薪水降序排列。</p>
</li>
</ul>
<h3 id="3-编程语言中的-sort-by（如-Python、Java）"><a href="#3-编程语言中的-sort-by（如-Python、Java）" class="headerlink" title="3. 编程语言中的 sort by（如 Python、Java）"></a>3. 编程语言中的 <code>sort by</code>（如 Python、Java）</h3><ul>
<li><p><strong>功能</strong>：在编程语言中，通常使用类似 <code>sort by</code> 的语法对集合或数据结构进行排序。</p>
</li>
<li><p><strong>应用场景</strong>：在程序中对内存中的数据进行排序。</p>
</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">employees = [</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;salary&quot;</span>: <span class="number">5000</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;salary&quot;</span>: <span class="number">6000</span>&#125;</span><br><span class="line">]</span><br><span class="line">sorted_employees = <span class="built_in">sorted</span>(employees, key=<span class="keyword">lambda</span> x: x[<span class="string">&quot;salary&quot;</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-主要区别总结"><a href="#4-主要区别总结" class="headerlink" title="4. 主要区别总结"></a>4. 主要区别总结</h3><table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>ORDER BY</strong></th>
<th><strong>SORT BY</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>排序范围</strong></td>
<td>全局排序</td>
<td>分区内排序（数据库）或局部排序（编程语言）</td>
</tr>
<tr>
<td><strong>数据分布</strong></td>
<td>需收集所有数据</td>
<td>分区处理，数据无需集中</td>
</tr>
<tr>
<td><strong>性能</strong></td>
<td>低（尤其在大数据量时）</td>
<td>高</td>
</tr>
<tr>
<td><strong>结果保证</strong></td>
<td>完全有序</td>
<td>分区内有序，全局不一定有序</td>
</tr>
<tr>
<td><strong>典型场景</strong></td>
<td>生成最终报表、严格有序输出</td>
<td>中间处理步骤、提升性能</td>
</tr>
</tbody></table>
<h3 id="5-使用建议"><a href="#5-使用建议" class="headerlink" title="5. 使用建议"></a>5. 使用建议</h3><ul>
<li><strong>需要全局排序</strong>：使用 <code>ORDER BY</code>，但注意大数据量下的性能问题。</li>
<li><strong>分布式环境优化</strong>：使用 <code>DISTRIBUTE BY + SORT BY</code> 替代 <code>ORDER BY</code>，提升性能。</li>
<li><strong>编程语言中排序</strong>：使用内置的排序函数，根据需求选择升序或降序。</li>
</ul>
<p>理解这些区别后，你可以根据具体场景选择合适的排序方式，在保证结果正确性的同时优化性能。</p>
<h2 id="13-cluster-by-和distribute-by-的区别和使用场景"><a href="#13-cluster-by-和distribute-by-的区别和使用场景" class="headerlink" title="13. cluster by 和distribute by 的区别和使用场景?"></a>13. cluster by 和distribute by 的区别和使用场景?</h2><p>在数据处理中，<code>CLUSTER BY</code> 和 <code>DISTRIBUTE BY</code> 是用于控制数据分布和分组的操作，主要在分布式计算框架（如 Hive、Spark SQL）中使用。它们的区别和应用场景如下：</p>
<h3 id="1-DISTRIBUTE-BY-的功能与场景"><a href="#1-DISTRIBUTE-BY-的功能与场景" class="headerlink" title="1. DISTRIBUTE BY 的功能与场景"></a><strong>1. <code>DISTRIBUTE BY</code> 的功能与场景</strong></h3><h4 id="功能"><a href="#功能" class="headerlink" title="功能"></a><strong>功能</strong></h4><ul>
<li><strong>数据分区</strong>：将数据按照指定的字段分发到不同的分区（Partition）或节点中，确保相同字段值的数据被分配到同一个分区。</li>
<li><strong>不保证排序</strong>：只控制数据的分布，不保证分区内数据有序。</li>
</ul>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a><strong>应用场景</strong></h4><ul>
<li><strong>优化后续操作</strong>：例如，在执行 <code>JOIN</code> 或 <code>GROUP BY</code> 前，通过 <code>DISTRIBUTE BY</code> 将关联键相同的数据分到同一节点，减少数据传输开销。</li>
<li><strong>与 <code>SORT BY</code> 配合</strong>：在每个分区内进行排序（需搭配 <code>SORT BY</code>）。</li>
</ul>
<h4 id="示例（Hive-语法）"><a href="#示例（Hive-语法）" class="headerlink" title="示例（Hive 语法）"></a><strong>示例（Hive 语法）</strong></h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">DISTRIBUTE <span class="keyword">BY</span> department;</span><br></pre></td></tr></table></figure>

<ul>
<li>数据会按 <code>department</code> 字段分发到不同分区，但每个分区内的数据是无序的。</li>
</ul>
<h3 id="2-CLUSTER-BY-的功能与场景"><a href="#2-CLUSTER-BY-的功能与场景" class="headerlink" title="2. CLUSTER BY 的功能与场景"></a><strong>2. <code>CLUSTER BY</code> 的功能与场景</strong></h3><h4 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a><strong>功能</strong></h4><ul>
<li><strong>数据分区 + 排序</strong>：相当于 <code>DISTRIBUTE BY</code> 和 <code>SORT BY</code> 的组合，既按指定字段分区，又在每个分区内按该字段排序。</li>
<li><strong>等价写法</strong>：<code>CLUSTER BY col</code> 等价于 <code>DISTRIBUTE BY col SORT BY col</code>。</li>
</ul>
<h4 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a><strong>应用场景</strong></h4><ul>
<li><strong>简化语法</strong>：当分区字段和排序字段相同时，使用 <code>CLUSTER BY</code> 更简洁。</li>
<li><strong>数据聚类</strong>：使相同字段值的数据聚集在一起，且有序排列，便于后续处理。</li>
</ul>
<h4 id="示例（Hive-语法）-1"><a href="#示例（Hive-语法）-1" class="headerlink" title="示例（Hive 语法）"></a><strong>示例（Hive 语法）</strong></h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">CLUSTER <span class="keyword">BY</span> department;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>等价于：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">DISTRIBUTE <span class="keyword">BY</span> department</span><br><span class="line">SORT <span class="keyword">BY</span> department;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-核心区别对比"><a href="#3-核心区别对比" class="headerlink" title="3. 核心区别对比"></a><strong>3. 核心区别对比</strong></h3><table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>DISTRIBUTE BY</strong></th>
<th><strong>CLUSTER BY</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>数据分布</strong></td>
<td>按字段分区，确保相同值在同一分区</td>
<td>同上</td>
</tr>
<tr>
<td><strong>分区内排序</strong></td>
<td>不保证排序</td>
<td>按相同字段排序（升序）</td>
</tr>
<tr>
<td><strong>语法复杂度</strong></td>
<td>需搭配 <code>SORT BY</code> 实现排序</td>
<td>一步完成分区和排序，更简洁</td>
</tr>
<tr>
<td><strong>典型场景</strong></td>
<td>优化 <code>JOIN</code>、分区后自定义排序</td>
<td>数据聚类、简化语法</td>
</tr>
</tbody></table>
<h3 id="4-使用建议"><a href="#4-使用建议" class="headerlink" title="4. 使用建议"></a><strong>4. 使用建议</strong></h3><ol>
<li><p><strong>只需分区</strong>：使用 <code>DISTRIBUTE BY</code>，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">DISTRIBUTE <span class="keyword">BY</span> department</span><br><span class="line">SORT <span class="keyword">BY</span> salary <span class="keyword">DESC</span>;  <span class="comment">-- 分区后按薪水排序</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>分区 + 排序（字段相同）</strong>：使用 <code>CLUSTER BY</code>，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">CLUSTER <span class="keyword">BY</span> department;  <span class="comment">-- 按部门分区并排序</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>分区 + 排序（字段不同）</strong>：使用 <code>DISTRIBUTE BY + SORT BY</code>，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> employees</span><br><span class="line">DISTRIBUTE <span class="keyword">BY</span> department</span><br><span class="line">SORT <span class="keyword">BY</span> salary <span class="keyword">DESC</span>;  <span class="comment">-- 按部门分区，每个分区内按薪水排序</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-性能考虑"><a href="#5-性能考虑" class="headerlink" title="5. 性能考虑"></a><strong>5. 性能考虑</strong></h3><ul>
<li><strong>数据倾斜</strong>：若字段值分布不均（如 <code>department</code> 中某部门数据过多），可能导致部分分区处理压力过大。</li>
<li><strong>分布式计算</strong>：合理使用 <code>DISTRIBUTE BY</code> 可减少 <code>SHUFFLE</code> 开销，但过度分区会增加任务数。</li>
</ul>
<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a><strong>总结</strong></h3><ul>
<li><strong>DISTRIBUTE BY</strong>：专注于数据分区，适用于优化数据分布。</li>
<li><strong>CLUSTER BY</strong>：分区 + 排序一步到位，适用于简化语法和数据聚类。</li>
</ul>
<p>根据具体需求选择，若需自定义排序规则或分区与排序字段不同，使用 <code>DISTRIBUTE BY + SORT BY</code>；否则使用 <code>CLUSTER BY</code> 更高效。</p>
<p>​                               </p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://cryingatnight.github.io">Yinjin Yao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://cryingatnight.github.io/2025/07/07/hadoop%E9%97%AE%E7%AD%94%E5%B0%8F%E6%B5%8B%E9%AA%8C/">https://cryingatnight.github.io/2025/07/07/hadoop%E9%97%AE%E7%AD%94%E5%B0%8F%E6%B5%8B%E9%AA%8C/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://cryingatnight.github.io" target="_blank">Yinjin Yao的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hadoop/">Hadoop</a></div><div class="post-share"><div class="social-share" data-image="/img/%E4%BA%94%E6%9D%A1%E6%82%9F.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/14/docker/" title="docker"><img class="cover" src="/img/lita5.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">docker</div></div><div class="info-2"><div class="info-item-1">docker介绍在安装部署程序过程中存在的问题：  版本依赖  典型案例：java环境有很多的jre版本   环境依赖  典型案例：oracle安装需要大量支撑包   安全风险  典型案例：多个应用无法安装一个服务器   性能瓶颈  典型案例：硬件设备扩展困难    什么是虚拟化技术在计算机技术中，虚拟化是一种资源管理技术，是将计算机的各种实体资源，如服务器，网络，内存以及存储等予以抽象转换后呈现出来，打破实体实体结构间不可切割的障碍，使用户可以用比原本的组态更好的方式来应用这些资源 docekr容器技术的发展 Docker是基于Go语言实现的开原容器项目它诞生于2013年年初.现在主流的操作系统包括引inux各大发行版本，MacOS,windows等都支持Docker..各大云服务提供商也纷纷推出了基于Docker的服务 Docker的构想是要实现”Build ship and run any App,Anywhere‘“即通过对应用的封装(packaging),分发(distribution),部署(deploymen).运行(runtime)生命周期进行管理，达到应用组件级...</div></div></div></a><a class="pagination-related" href="/2025/06/27/Hive/" title="Hive"><img class="cover" src="/img/lita2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hive</div></div><div class="info-2"><div class="info-item-1">什么是HIVEHive 是基于 Hadoop 的一个数据仓库工具。以下是具体介绍:  功能特点：Hive 可以将结构化的数据文件映射为一张数据库表，并提供完整的 SQL 查询功能，能将 SQL 语句转换为 MapReduce 任务进行运行。它允许熟悉 SQL 的用户方便地查询数据，也支持熟悉 MapReduce 的开发者自定义 mapper 和 reducer，以处理复杂的分析工作。 优势：学习成本低，通过类 SQL 语句可快速实现简单的 MapReduce 统计，无需开发专门的 MapReduce 应用，十分适合数据仓库的统计分析。 应用场景：常用于对时效性要求不高的数据分析场景。由于 Hive 底层依赖 Hadoop 的 HDFS 存储数据，利用 MapReduce 进行计算，因此能够处理大规模的数据，在处理海量结构化日志的数据统计等方面应用广泛。 与数据库的区别： 数据库一般用于在线应用，支持对某一行或某些行数据的更新、删除等操作，采用 “写时模式”，数据加载慢但查询快。 而 Hive 不支持对具体行的操作，也不支持事务和索引，采用 “读时模式”，适合处理非结构化或存储模式...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/24/Hadoop/" title="Hadoop"><img class="cover" src="/img/lita9.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">Hadoop</div></div><div class="info-2"><div class="info-item-1">Hadoop简介 Hadoop是一个由Apache基金会所开发的分布式系统基础架构 主要解决，海量数据的存储和海量数据的分析计算问题 广义上来说，Hadoop通常是指一个更广泛的概念-Hadoop生态圈  分布式存储Hadoop 的分布式存储主要基于 HDFS（分布式文件系统）:HDFS将数据分割成多个数据块（block），这些数据块分散存储在集群中的不同节点上。每个数据块会有多个副本，通常默认是 3 个副本.采用分布式存储在不同的节点上，提高了数据的可靠性和容错性。  Hadoop的分布式核心组件是MapReduce编程模型:在MapReduce任务中，数据被切分为多个任务，每个任务由或多个节点并行。每个节点负责将输入数据映射为键-值对生成中间结果。最后，中间结果按照键的排序进行合并和归并。   Hadoop组件(面试重点)hadoop问答小测验 HDFS 架构概述HDFS组件用于存储数据,主要由NameNode,DataNode,SecondaryNameNode 组成  NameNode (nn): 存储文件的元数据，如文件名，文件目录结构，文件属性 (生成时间、副本数、文...</div></div></div></a><a class="pagination-related" href="/2025/08/10/datax/" title="datax"><img class="cover" src="/img/lita6.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-10</div><div class="info-item-2">datax</div></div><div class="info-2"><div class="info-item-1">DataX简介DataX概述DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。 datax源码地址：https://github.com/alibaba/DataX DataX支持的数据源DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图。    类型 数据源 Reader(读) Writer(写) 文档    RDBMS 关系型数据库 MySQL √ √ 读 、写    Oracle √ √ 读 、写    OceanBase √ √ 读 、写    SQLServer √ √ 读 、写    PostgreSQL √ √ 读 、写    DRDS √ √ 读 、写    Kingbase √ √ 读 、写    通用RDBMS(支持所有关系型数据库) √ √ 读 、写   阿里云数仓数据存储 ODPS √ √ 读 、写    ADB  √ 写    A...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/headimage.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Yinjin Yao</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/cryingatnight/cryingatnight.github.io"><i class="fab fa-github"></i><span>关注</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/cryingatnight/cryingatnight.github.io" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1816192779@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#hadoop%E9%97%AE%E7%AD%94%E5%B0%8F%E6%B5%8B%E9%AA%8C"><span class="toc-number">1.</span> <span class="toc-text">hadoop问答小测验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-hadoop%E7%BB%84%E4%BB%B6%E6%9C%89%E5%93%AA%E4%BA%9B-%E5%88%86%E5%88%AB%E6%9C%89%E4%BB%80%E4%B9%88%E5%8A%9F%E8%83%BD"><span class="toc-number">1.1.</span> <span class="toc-text">1. hadoop组件有哪些?分别有什么功能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">1.1.1.</span> <span class="toc-text">一、核心组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%BB%84%E4%BB%B6"><span class="toc-number">1.1.2.</span> <span class="toc-text">二、生态系统组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Hive%E2%80%94%E2%80%94-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E5%85%B7"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">1. Hive—— 数据仓库工具</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Sqoop%E2%80%94%E2%80%94-%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">2. Sqoop—— 数据迁移工具</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Spark%E2%80%94%E2%80%94-%E5%BF%AB%E9%80%9F%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%EF%BC%88%E7%94%9F%E6%80%81%E5%85%B3%E8%81%94%E7%BB%84%E4%BB%B6%EF%BC%89"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">3. Spark—— 快速计算引擎（生态关联组件）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%9A%84%E7%BB%84%E4%BB%B6%E6%98%AF%E4%BB%80%E4%B9%88-%E6%9C%89%E5%93%AA%E4%BA%9B%E8%BF%9B%E7%A8%8B-%E8%BF%9B%E7%A8%8B%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.2.</span> <span class="toc-text">2. 分布式存储的组件是什么 ?有哪些进程? 进程的作用是什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81HDFS-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">一、HDFS 核心组件与进程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-NameNode%EF%BC%88%E4%B8%BB%E8%8A%82%E7%82%B9%E8%BF%9B%E7%A8%8B%EF%BC%89"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">1. NameNode（主节点进程）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-DataNode%EF%BC%88%E4%BB%8E%E8%8A%82%E7%82%B9%E8%BF%9B%E7%A8%8B%EF%BC%89"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">2. DataNode（从节点进程）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SecondaryNameNode%EF%BC%88%E8%BE%85%E5%8A%A9%E5%90%8D%E7%A7%B0%E8%8A%82%E7%82%B9%E8%BF%9B%E7%A8%8B%EF%BC%89"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">3. SecondaryNameNode（辅助名称节点进程）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-JournalNode%EF%BC%88%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9B%E7%A8%8B%EF%BC%8CHDFS-2-0-%EF%BC%89"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">4. JournalNode（高可用进程，HDFS 2.0+）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-ZKFailoverController%EF%BC%88%E9%AB%98%E5%8F%AF%E7%94%A8%E8%BF%9B%E7%A8%8B%EF%BC%8CHDFS-2-0-%EF%BC%89"><span class="toc-number">1.2.1.5.</span> <span class="toc-text">5. ZKFailoverController（高可用进程，HDFS 2.0+）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81HDFS-%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%88HA%EF%BC%89%E6%9E%B6%E6%9E%84%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.2.2.</span> <span class="toc-text">二、HDFS 高可用（HA）架构进程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Active-NameNode"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">1. Active NameNode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Standby-NameNode"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">2. Standby NameNode</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-ZooKeeper-%E9%9B%86%E7%BE%A4"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">3. ZooKeeper 集群</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.2.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E4%BD%BF%E7%94%A8%E4%BB%80%E4%B9%88%E7%BB%84%E4%BB%B6-%E8%AF%A5%E7%BB%84%E4%BB%B6%E6%9C%89%E5%93%AA%E4%BA%9B%E8%BF%9B%E7%A8%8B-%E6%AF%8F%E4%B8%AA%E8%BF%9B%E7%A8%8B%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.3.</span> <span class="toc-text">3. 资源调度使用什么组件? 该组件有哪些进程?每个进程的作用是什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81YARN-%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E4%B8%8E%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.3.1.</span> <span class="toc-text">一、YARN 的核心组件与进程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-ResourceManager%EF%BC%88RM%EF%BC%8C%E4%B8%BB%E8%8A%82%E7%82%B9%E8%BF%9B%E7%A8%8B%EF%BC%89"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">1. ResourceManager（RM，主节点进程）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-NodeManager%EF%BC%88NM%EF%BC%8C%E4%BB%8E%E8%8A%82%E7%82%B9%E8%BF%9B%E7%A8%8B%EF%BC%89"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">2. NodeManager（NM，从节点进程）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-ApplicationMaster%EF%BC%88AM%EF%BC%8C%E6%AF%8F%E4%B8%AA%E5%BA%94%E7%94%A8%E7%8B%AC%E6%9C%89%E7%9A%84%E8%BF%9B%E7%A8%8B%EF%BC%89"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">3. ApplicationMaster（AM，每个应用独有的进程）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Container%EF%BC%88%E8%B5%84%E6%BA%90%E5%AE%B9%E5%99%A8%EF%BC%89"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">4. Container（资源容器）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81YARN-%E7%9A%84%E8%B0%83%E5%BA%A6%E5%99%A8%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.3.2.</span> <span class="toc-text">二、YARN 的调度器类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Capacity-Scheduler%EF%BC%89"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">1. 容量调度器（Capacity Scheduler）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Fair-Scheduler%EF%BC%89"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">2. 公平调度器（Fair Scheduler）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B1%82%E6%AC%A1%E8%B0%83%E5%BA%A6%E5%99%A8%EF%BC%88Hierarchical-Scheduler%EF%BC%89"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">3. 层次调度器（Hierarchical Scheduler）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81YARN-%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%88HA%EF%BC%89%E6%9E%B6%E6%9E%84%E8%BF%9B%E7%A8%8B"><span class="toc-number">1.3.3.</span> <span class="toc-text">三、YARN 高可用（HA）架构进程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Active-ResourceManager"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">1. Active ResourceManager</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Standby-ResourceManager"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">2. Standby ResourceManager</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-ZooKeeper-%E9%9B%86%E7%BE%A4-1"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">3. ZooKeeper 集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-ZKFC%EF%BC%88ZooKeeper-Failover-Controller%EF%BC%89"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">4. ZKFC（ZooKeeper Failover Controller）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">1.3.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%88%86%E4%B8%BA%E5%87%A0%E4%B8%AA%E6%AD%A5%E9%AA%A4-shuffle%E6%9C%89%E5%87%A0%E4%B8%AA%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.4.</span> <span class="toc-text">4. 分布式计算分为几个步骤? shuffle有几个步骤?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E7%9A%84%E5%85%B8%E5%9E%8B%E6%AD%A5%E9%AA%A4%EF%BC%88%E4%BB%A5-MapReduce-%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="toc-number">1.4.1.</span> <span class="toc-text">一、分布式计算的典型步骤（以 MapReduce 为例）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87%EF%BC%88Data-Splitting%EF%BC%89"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">1. 数据分片（Data Splitting）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Map-%E9%98%B6%E6%AE%B5"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">2. Map 阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Shuffle-%E9%98%B6%E6%AE%B5"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">3. Shuffle 阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Reduce-%E9%98%B6%E6%AE%B5"><span class="toc-number">1.4.1.4.</span> <span class="toc-text">4. Reduce 阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E7%BB%93%E6%9E%9C%E5%90%88%E5%B9%B6%E4%B8%8E%E5%AD%98%E5%82%A8"><span class="toc-number">1.4.1.5.</span> <span class="toc-text">5. 结果合并与存储</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Shuffle-%E9%98%B6%E6%AE%B5%E7%9A%84%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%EF%BC%88%E4%BB%A5-MapReduce-%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="toc-number">1.4.2.</span> <span class="toc-text">二、Shuffle 阶段的详细步骤（以 MapReduce 为例）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Map-%E7%AB%AF%E7%9A%84-Shuffle-%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">Map 端的 Shuffle 步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reduce-%E7%AB%AF%E7%9A%84-Shuffle-%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">Reduce 端的 Shuffle 步骤</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-hdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.5.</span> <span class="toc-text">5. hdfs文件系统的读取数据流程是什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E8%B5%B7%E8%AF%BB%E5%8F%96%E8%AF%B7%E6%B1%82"><span class="toc-number">1.5.1.</span> <span class="toc-text">1. 客户端发起读取请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%8E%B7%E5%8F%96%E6%96%87%E4%BB%B6%E5%85%83%E6%95%B0%E6%8D%AE%EF%BC%88%E4%B8%8E-NameNode-%E4%BA%A4%E4%BA%92%EF%BC%89"><span class="toc-number">1.5.2.</span> <span class="toc-text">2. 获取文件元数据（与 NameNode 交互）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BE%93%E5%85%A5%E6%B5%81"><span class="toc-number">1.5.3.</span> <span class="toc-text">3. 客户端初始化输入流</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%EF%BC%88%E4%B8%8E-DataNode-%E4%BA%A4%E4%BA%92%EF%BC%89"><span class="toc-number">1.5.4.</span> <span class="toc-text">4. 客户端读取数据（与 DataNode 交互）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%AE%B9%E9%94%99%E5%A4%84%E7%90%86"><span class="toc-number">1.5.5.</span> <span class="toc-text">5. 容错处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AF%BB%E5%8F%96%E5%AE%8C%E6%88%90%EF%BC%8C%E5%85%B3%E9%97%AD%E6%B5%81"><span class="toc-number">1.5.6.</span> <span class="toc-text">6. 读取完成，关闭流</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">1.5.7.</span> <span class="toc-text">核心特点总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-hdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%86%99%E6%96%87%E4%BB%B6%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.6.</span> <span class="toc-text">6. hdfs文件系统的写文件流程是什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E8%B5%B7%E5%86%99%E8%AF%B7%E6%B1%82"><span class="toc-number">1.6.1.</span> <span class="toc-text">1. 客户端发起写请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%90%91-NameNode-%E8%AF%B7%E6%B1%82%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6"><span class="toc-number">1.6.2.</span> <span class="toc-text">2. 向 NameNode 请求创建文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BE%93%E5%87%BA%E6%B5%81%E5%B9%B6%E8%8E%B7%E5%8F%96-DataNode-%E5%88%97%E8%A1%A8"><span class="toc-number">1.6.3.</span> <span class="toc-text">3. 客户端初始化输出流并获取 DataNode 列表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%BB%BA%E7%AB%8B%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E7%AE%A1%E9%81%93%EF%BC%88Pipeline%EF%BC%89"><span class="toc-number">1.6.4.</span> <span class="toc-text">4. 建立数据传输管道（Pipeline）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%EF%BC%88%E5%88%86%E5%9D%97-%E5%88%86%E6%95%B0%E6%8D%AE%E5%8C%85%EF%BC%89"><span class="toc-number">1.6.5.</span> <span class="toc-text">5. 客户端写入数据（分块 + 分数据包）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%95%B0%E6%8D%AE%E5%9D%97%E5%86%99%E5%85%A5%E5%AE%8C%E6%88%90%EF%BC%8C%E8%AF%B7%E6%B1%82%E6%96%B0%E5%9D%97"><span class="toc-number">1.6.6.</span> <span class="toc-text">6. 数据块写入完成，请求新块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%86%99%E5%85%A5%E5%AE%8C%E6%88%90%EF%BC%8C%E5%85%B3%E9%97%AD%E6%B5%81%E5%B9%B6%E7%A1%AE%E8%AE%A4"><span class="toc-number">1.6.7.</span> <span class="toc-text">7. 写入完成，关闭流并确认</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E5%A4%84%E7%90%86%EF%BC%88%E5%85%B3%E9%94%AE%E6%9C%BA%E5%88%B6%EF%BC%89"><span class="toc-number">1.6.8.</span> <span class="toc-text">容错处理（关键机制）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9%E6%80%BB%E7%BB%93-1"><span class="toc-number">1.6.9.</span> <span class="toc-text">核心特点总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-hdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-namenode%E7%9A%84%E6%9B%B4%E6%96%B0%E6%B5%81%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.7.</span> <span class="toc-text">7. hdfs文件系统 namenode的更新流程是什么?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%85%83%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AD%98%E5%82%A8%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.7.1.</span> <span class="toc-text">一、元数据的存储形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%85%83%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E7%9A%84%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%EF%BC%88%E4%BB%A5-%E2%80%9C%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5%E2%80%9D-%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="toc-number">1.7.2.</span> <span class="toc-text">二、元数据更新的核心流程（以 “文件写入” 为例）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E8%B5%B7%E6%93%8D%E4%BD%9C%E8%AF%B7%E6%B1%82"><span class="toc-number">1.7.2.1.</span> <span class="toc-text">1. 客户端发起操作请求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-NameNode-%E9%AA%8C%E8%AF%81%E6%93%8D%E4%BD%9C%E5%90%88%E6%B3%95%E6%80%A7"><span class="toc-number">1.7.2.2.</span> <span class="toc-text">2. NameNode 验证操作合法性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%AE%B0%E5%BD%95%E6%93%8D%E4%BD%9C%E5%88%B0-EditLog%EF%BC%88%E9%A2%84%E5%86%99%E6%97%A5%E5%BF%97%EF%BC%8CWrite-Ahead-Log%EF%BC%89"><span class="toc-number">1.7.2.3.</span> <span class="toc-text">3. 记录操作到 EditLog（预写日志，Write-Ahead Log）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%9B%B4%E6%96%B0%E5%86%85%E5%AD%98%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.7.2.4.</span> <span class="toc-text">4. 更新内存元数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%89%B9%E7%82%B9%E6%80%BB%E7%BB%93-2"><span class="toc-number">1.7.3.</span> <span class="toc-text">核心特点总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E6%95%B4%E4%B8%AAmapreduce%E9%98%B6%E6%AE%B5%E7%BB%8F%E5%8E%86%E5%87%A0%E6%AC%A1%E6%8E%92%E5%BA%8F-%E5%88%86%E5%88%AB%E5%9C%A8%E5%93%AA%E4%BA%9B%E9%98%B6%E6%AE%B5"><span class="toc-number">1.8.</span> <span class="toc-text">8. 整个mapreduce阶段经历几次排序?分别在哪些阶段?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E"><span class="toc-number">1.8.1.</span> <span class="toc-text">补充说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-hive%E4%B8%AD%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB-%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="toc-number">1.9.</span> <span class="toc-text">9. hive中内部表和外部表有什么区别? 分区表和分桶表有什么区别?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%86%85%E9%83%A8%E8%A1%A8%EF%BC%88Managed-Table%EF%BC%89-vs-%E5%A4%96%E9%83%A8%E8%A1%A8%EF%BC%88External-Table%EF%BC%89"><span class="toc-number">1.9.1.</span> <span class="toc-text">一、内部表（Managed Table） vs 外部表（External Table）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8F"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">1. 数据管理方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E8%AF%AD%E6%B3%95%E5%B7%AE%E5%BC%82"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">2. 创建语法差异</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.9.1.3.</span> <span class="toc-text">3. 使用场景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%88%86%E5%8C%BA%E8%A1%A8%EF%BC%88Partitioned-Table%EF%BC%89-vs-%E5%88%86%E6%A1%B6%E8%A1%A8%EF%BC%88Bucketed-Table%EF%BC%89"><span class="toc-number">1.9.2.</span> <span class="toc-text">二、分区表（Partitioned Table） vs 分桶表（Bucketed Table）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87%E6%96%B9%E5%BC%8F"><span class="toc-number">1.9.2.1.</span> <span class="toc-text">1. 数据组织方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E8%AF%AD%E6%B3%95%E5%B7%AE%E5%BC%82-1"><span class="toc-number">1.9.2.2.</span> <span class="toc-text">2. 创建语法差异</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF-1"><span class="toc-number">1.9.2.3.</span> <span class="toc-text">3. 使用场景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93"><span class="toc-number">1.9.3.</span> <span class="toc-text">三、核心区别总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E5%BB%BA%E8%AE%AE"><span class="toc-number">1.9.4.</span> <span class="toc-text">四、最佳实践建议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E6%98%AF%E4%BB%80%E4%B9%88-%E5%A6%82%E4%BD%95%E8%A7%84%E9%81%BF%E5%92%8C%E5%A4%84%E7%90%86"><span class="toc-number">1.10.</span> <span class="toc-text">10. 数据倾斜是什么? 如何规避和处理?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E8%A1%A8%E7%8E%B0%E4%B8%8E%E5%BD%B1%E5%93%8D"><span class="toc-number">1.10.1.</span> <span class="toc-text">数据倾斜的表现与影响</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E5%B8%B8%E8%A7%81%E5%9C%BA%E6%99%AF"><span class="toc-number">1.10.2.</span> <span class="toc-text">数据倾斜的常见场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E8%A7%84%E9%81%BF%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8B%E5%89%8D%E9%A2%84%E9%98%B2%EF%BC%89"><span class="toc-number">1.10.3.</span> <span class="toc-text">数据倾斜的规避方法（事前预防）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A%E8%BF%87%E6%BB%A4-%E6%B8%85%E6%B4%97%E6%97%A0%E6%95%88%E6%95%B0%E6%8D%AE"><span class="toc-number">1.10.3.1.</span> <span class="toc-text">1. 数据预处理：过滤 &#x2F; 清洗无效数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%8B%86%E5%88%86%E5%A4%A7-key%EF%BC%9A%E5%B0%86%E9%9B%86%E4%B8%AD%E7%9A%84-key-%E5%88%86%E6%95%A3%E5%8C%96"><span class="toc-number">1.10.3.2.</span> <span class="toc-text">2. 拆分大 key：将集中的 key 分散化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%AE%A1%E7%AE%97%E9%80%BB%E8%BE%91%EF%BC%9A%E9%81%BF%E5%85%8D%E4%B8%8D%E5%BF%85%E8%A6%81%E7%9A%84-shuffle"><span class="toc-number">1.10.3.3.</span> <span class="toc-text">3. 选择合适的计算逻辑：避免不必要的 shuffle</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8B%E4%B8%AD%E8%A7%A3%E5%86%B3%EF%BC%89"><span class="toc-number">1.10.4.</span> <span class="toc-text">数据倾斜的处理方法（事中解决）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%A4%A7-key-%E6%8B%86%E5%88%86%EF%BC%88%E5%8A%A0%E7%9B%90%E6%B3%95%EF%BC%89"><span class="toc-number">1.10.4.1.</span> <span class="toc-text">1. 大 key 拆分（加盐法）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%B0%83%E6%95%B4-shuffle-%E5%88%86%E5%8C%BA%E6%95%B0"><span class="toc-number">1.10.4.2.</span> <span class="toc-text">2. 调整 shuffle 分区数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%A1%86%E6%9E%B6%E7%BA%A7%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-number">1.10.4.3.</span> <span class="toc-text">3. 框架级参数优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%BF%87%E6%BB%A4%E6%88%96%E5%8D%95%E7%8B%AC%E5%A4%84%E7%90%86%E5%A4%A7-key"><span class="toc-number">1.10.4.4.</span> <span class="toc-text">4. 过滤或单独处理大 key</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">1.10.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8hdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%8A%E4%BC%A0%E6%88%96%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE-%E5%85%B3%E9%97%ADhdfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.11.</span> <span class="toc-text">11. 如何使用hdfs文件系统上传或下载数据? 关闭hdfs文件系统安全模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81HDFS-%E6%95%B0%E6%8D%AE%E4%B8%8A%E4%BC%A0%E4%B8%8E%E4%B8%8B%E8%BD%BD"><span class="toc-number">1.11.1.</span> <span class="toc-text">一、HDFS 数据上传与下载</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B8%8A%E4%BC%A0%E6%95%B0%E6%8D%AE%E5%88%B0-HDFS%EF%BC%88%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%88%B0-HDFS%EF%BC%89"><span class="toc-number">1.11.1.1.</span> <span class="toc-text">1. 上传数据到 HDFS（从本地到 HDFS）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%8E-HDFS-%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%EF%BC%88%E4%BB%8E-HDFS-%E5%88%B0%E6%9C%AC%E5%9C%B0%EF%BC%89"><span class="toc-number">1.11.1.2.</span> <span class="toc-text">2. 从 HDFS 下载数据（从 HDFS 到本地）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8-HDFS-%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4"><span class="toc-number">1.11.1.3.</span> <span class="toc-text">3. 其他常用 HDFS 操作命令</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%85%B3%E9%97%AD-HDFS-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.11.2.</span> <span class="toc-text">二、关闭 HDFS 安全模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E7%AE%80%E4%BB%8B"><span class="toc-number">1.11.2.1.</span> <span class="toc-text">1. 安全模式简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E7%8A%B6%E6%80%81"><span class="toc-number">1.11.2.2.</span> <span class="toc-text">2. 查看当前安全模式状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%89%8B%E5%8A%A8%E5%85%B3%E9%97%AD%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%EF%BC%88%E9%9C%80%E7%AE%A1%E7%90%86%E5%91%98%E6%9D%83%E9%99%90%EF%BC%89"><span class="toc-number">1.11.2.3.</span> <span class="toc-text">3. 手动关闭安全模式（需管理员权限）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%BC%BA%E5%88%B6%E9%80%80%E5%87%BA%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%EF%BC%88%E8%B0%A8%E6%85%8E%E4%BD%BF%E7%94%A8%EF%BC%89"><span class="toc-number">1.11.2.4.</span> <span class="toc-text">4. 强制退出安全模式（谨慎使用）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E8%BF%9B%E5%85%A5%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%EF%BC%88%E7%94%A8%E4%BA%8E%E7%BB%B4%E6%8A%A4%EF%BC%89"><span class="toc-number">1.11.2.5.</span> <span class="toc-text">5. 进入安全模式（用于维护）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">1.11.3.</span> <span class="toc-text">三、常见问题与解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E6%97%B6%E6%8F%90%E7%A4%BA-%E2%80%9C%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E5%BC%80%E5%90%AF%E2%80%9D"><span class="toc-number">1.11.3.1.</span> <span class="toc-text">1. 上传文件时提示 “安全模式开启”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%E6%97%A0%E6%B3%95%E8%87%AA%E5%8A%A8%E9%80%80%E5%87%BA"><span class="toc-number">1.11.3.2.</span> <span class="toc-text">2. 安全模式无法自动退出</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98"><span class="toc-number">1.11.3.3.</span> <span class="toc-text">3. 权限问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-number">1.11.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-order-by-%E5%92%8Csort-by-%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.12.</span> <span class="toc-text">12. order by 和sort by 的区别和使用场景?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-SQL-%E4%B8%AD%E7%9A%84-ORDER-BY"><span class="toc-number">1.12.1.</span> <span class="toc-text">1. SQL 中的 ORDER BY</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-SQL-%E4%B8%AD%E7%9A%84-SORT-BY%EF%BC%88%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E5%BA%93%E6%94%AF%E6%8C%81%EF%BC%89"><span class="toc-number">1.12.2.</span> <span class="toc-text">2. SQL 中的 SORT BY（部分数据库支持）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%9A%84-sort-by%EF%BC%88%E5%A6%82-Python%E3%80%81Java%EF%BC%89"><span class="toc-number">1.12.3.</span> <span class="toc-text">3. 编程语言中的 sort by（如 Python、Java）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%B8%BB%E8%A6%81%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93"><span class="toc-number">1.12.4.</span> <span class="toc-text">4. 主要区别总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E4%BD%BF%E7%94%A8%E5%BB%BA%E8%AE%AE"><span class="toc-number">1.12.5.</span> <span class="toc-text">5. 使用建议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-cluster-by-%E5%92%8Cdistribute-by-%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.13.</span> <span class="toc-text">13. cluster by 和distribute by 的区别和使用场景?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-DISTRIBUTE-BY-%E7%9A%84%E5%8A%9F%E8%83%BD%E4%B8%8E%E5%9C%BA%E6%99%AF"><span class="toc-number">1.13.1.</span> <span class="toc-text">1. DISTRIBUTE BY 的功能与场景</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD"><span class="toc-number">1.13.1.1.</span> <span class="toc-text">功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.13.1.2.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%88Hive-%E8%AF%AD%E6%B3%95%EF%BC%89"><span class="toc-number">1.13.1.3.</span> <span class="toc-text">示例（Hive 语法）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-CLUSTER-BY-%E7%9A%84%E5%8A%9F%E8%83%BD%E4%B8%8E%E5%9C%BA%E6%99%AF"><span class="toc-number">1.13.2.</span> <span class="toc-text">2. CLUSTER BY 的功能与场景</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD-1"><span class="toc-number">1.13.2.1.</span> <span class="toc-text">功能</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-1"><span class="toc-number">1.13.2.2.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%EF%BC%88Hive-%E8%AF%AD%E6%B3%95%EF%BC%89-1"><span class="toc-number">1.13.2.3.</span> <span class="toc-text">示例（Hive 语法）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E5%AF%B9%E6%AF%94"><span class="toc-number">1.13.3.</span> <span class="toc-text">3. 核心区别对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BD%BF%E7%94%A8%E5%BB%BA%E8%AE%AE"><span class="toc-number">1.13.4.</span> <span class="toc-text">4. 使用建议</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%80%A7%E8%83%BD%E8%80%83%E8%99%91"><span class="toc-number">1.13.5.</span> <span class="toc-text">5. 性能考虑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-number">1.13.6.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/25/RAG%E6%A3%80%E7%B4%A2%E7%94%9F%E6%88%90/" title="RAG检索生成"><img src="/img/lita8.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAG检索生成"/></a><div class="content"><a class="title" href="/2025/08/25/RAG%E6%A3%80%E7%B4%A2%E7%94%9F%E6%88%90/" title="RAG检索生成">RAG检索生成</a><time datetime="2025-09-18T01:53:30.365Z" title="更新于 2025-09-18 09:53:30">2025-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/14/Dify%E6%95%99%E7%A8%8B/" title="Dify教程"><img src="/img/lita2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dify教程"/></a><div class="content"><a class="title" href="/2025/09/14/Dify%E6%95%99%E7%A8%8B/" title="Dify教程">Dify教程</a><time datetime="2025-09-14T08:35:36.138Z" title="更新于 2025-09-14 16:35:36">2025-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/25/frp-%E6%9C%8D%E5%8A%A1%E5%99%A8-%E5%AE%9E%E7%8E%B0%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/" title="frp+服务器:实现内网穿透"><img src="/img/lita4.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="frp+服务器:实现内网穿透"/></a><div class="content"><a class="title" href="/2025/07/25/frp-%E6%9C%8D%E5%8A%A1%E5%99%A8-%E5%AE%9E%E7%8E%B0%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/" title="frp+服务器:实现内网穿透">frp+服务器:实现内网穿透</a><time datetime="2025-09-10T03:02:30.433Z" title="更新于 2025-09-10 11:02:30">2025-09-10</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2025 By Yinjin Yao</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.2</a></span></div><div class="footer_custom_text">感谢阅读</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="输入以搜索内容..." type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>